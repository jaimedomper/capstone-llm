{
    "title": "How to run DBT in airflow without copying our repo",
    "question_body": "<p>We use airflow to orchestrate our workflows, and dbt with bigquery for our daily transformations in BigQuery. We have two separate git repos, one for our dbt project and a separate one for airflow.</p>\n<p>It seems the simplest approach to scheduling our daily <code>run dbt</code> seems to be a <code>BashOperator</code> in airflow. However, to schedule DBT to run with Airflow, it seems like our entire DBT project would need to be nested inside of our Airflow project, that way we can point to it for our <code>dbt run</code> bash command?</p>\n<p>Is it possible to trigger our <code>dbt run</code> and <code>dbt test</code> without moving our DBT directory inside of our Airflow directory? With the <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"nofollow noreferrer\">airflow-dbt package</a>,  for the <code>dir</code> in the <code>default_args</code>, maybe it is possible to point to the gibhub link for the DBT project here?</p>\n",
    "answer_body": "<p>My advice would be to leave your dbt and airflow codebases separated.\nThere is indeed a better way:</p>\n<ol>\n<li>dockerise your dbt project in a simple python-based image where you COPY the codebase</li>\n<li>push that to DockerHub or ECR or any other docker repository that you are using</li>\n<li>use the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\"><code>DockerOperator</code></a> in your airflow DAG to run that docker image with your dbt code</li>\n</ol>\n<p>I'm assuming that you use the airflow LocalExecutor here and that you want to execute your <code>dbt run</code> workload on the server where airflow is running. If that's not the case and that you have access to a Kubernetes cluster, I would suggest instead to use the <code>KubernetesPodOperator</code>.</p>\n"
}