{
    "items": [
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 20309608,
                "reputation": 755,
                "user_id": 14897019,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp4OZMeeKWelZEs8Li6wXDKHVQzXTekMxiYnx5Oh=k-s256",
                "display_name": "Pierre-Alexandre",
                "link": "https://stackoverflow.com/users/14897019/pierre-alexandre"
            },
            "is_answered": true,
            "view_count": 49706,
            "accepted_answer_id": 72799561,
            "answer_count": 3,
            "score": 20,
            "last_activity_date": 1711621053,
            "creation_date": 1656495908,
            "question_id": 72799237,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72799237/only-run-a-set-of-models",
            "title": "Only run a set of models?",
            "body": "<p>I started to migrate some of your transformations jobs to DBT. As you can see on the image bellow, there is usually 1 to 2 transformations before to have our final table (up to 5 transformations in some cases).</p>\n<p>What I am trying to achieve is to do dbt run only for a set on linked model. For instance, <code>sales_prediction</code> and <code>forecast</code>.  I am currently able to run either for everything with <code>dbt run o</code>r just speficif model using <code>dbt run --select model_name</code></p>\n<p><a href=\"https://i.sstatic.net/s4jl5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/s4jl5.png\" alt=\"enter image description here\" /></a></p>\n"
        },
        {
            "tags": [
                "postgresql",
                "dbt"
            ],
            "owner": {
                "account_id": 18804422,
                "reputation": 181,
                "user_id": 13714082,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/9bd56a8002be346b2f8969ba91e006fb?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Sara",
                "link": "https://stackoverflow.com/users/13714082/sara"
            },
            "is_answered": true,
            "view_count": 18085,
            "answer_count": 1,
            "score": 17,
            "last_activity_date": 1691687274,
            "creation_date": 1663668003,
            "last_edit_date": 1691687274,
            "question_id": 73784913,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73784913/dbt-ref-vs-source",
            "title": "dbt ref() vs source()",
            "body": "<p>I\u2019m trying to make one model in dbt depend on another (trying to run the second model after the first one is completely finished), but I\u2019m confused, when to use <code>ref()</code> or <code>source()</code>?</p>\n<p>What is the difference between them?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 9237891,
                "reputation": 180,
                "user_id": 6861925,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/1709350879390046/picture?type=large",
                "display_name": "Willie Chen",
                "link": "https://stackoverflow.com/users/6861925/willie-chen"
            },
            "is_answered": true,
            "view_count": 18464,
            "answer_count": 4,
            "score": 17,
            "last_activity_date": 1625265473,
            "creation_date": 1595271657,
            "last_edit_date": 1595790121,
            "question_id": 63002171,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63002171/can-dbt-connect-to-different-databases-in-the-same-project",
            "title": "Can dbt connect to different databases in the same project?",
            "body": "<p>It seems dbt only works for a single database.</p>\n<p>If my data is in a different database, will that still work? For example, if my datalake is using delta, but I want to run dbt using Redshift, would dbt still work for this case?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 10701804,
                "reputation": 263,
                "user_id": 7876879,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ef2122792a49d44a16700b79ad4152eb?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Even",
                "link": "https://stackoverflow.com/users/7876879/even"
            },
            "is_answered": true,
            "view_count": 16012,
            "accepted_answer_id": 69550357,
            "answer_count": 1,
            "score": 16,
            "last_activity_date": 1634104357,
            "creation_date": 1634103783,
            "question_id": 69550294,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69550294/how-to-setup-dbt-ui-for-data-lineage",
            "title": "How to setup dbt UI for data lineage?",
            "body": "<p>I'm new to the dbt and I'm planning to use dbt cli. One question is how to setup the dbt ui and have such a data lineage graph? I didn't find how to do it here with cli <a href=\"https://docs.getdbt.com/tutorial/create-a-project-dbt-cli\" rel=\"noreferrer\">https://docs.getdbt.com/tutorial/create-a-project-dbt-cli</a>.</p>\n<p><a href=\"https://i.sstatic.net/xrBa2.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xrBa2.png\" alt=\"enter image description here\" /></a></p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 8495759,
                "reputation": 285,
                "user_id": 6370402,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/405d8de362cab93c0f8c40dfa1d95f96?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Catazza",
                "link": "https://stackoverflow.com/users/6370402/catazza"
            },
            "is_answered": true,
            "view_count": 6364,
            "accepted_answer_id": 69849432,
            "answer_count": 1,
            "score": 16,
            "last_activity_date": 1636094292,
            "creation_date": 1634827884,
            "question_id": 69664170,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69664170/dbt-custom-schema-using-folder-structure",
            "title": "DBT custom schema using folder structure",
            "body": "<p>is there a way in DBT to create custom schemas for a model in a derived way by looking at the folder structure?</p>\n<p>For example, say this is my structure:</p>\n<pre><code>models\n\u2514-- product1\n    \u2514-- team1\n    |   \u2514-- model1.sql\n    \u2514-- team2\n        \u2514-- model2.sql\n</code></pre>\n<p>In this case, model1.sql would be created in the schema <code>product1_team1</code> whereas model2.sql would be created in the schema <code>product1_team2</code>. I guess I can specify those &quot;by hand&quot; in the <code>dbt_project.yml</code> file, but I was wondering if there was a way to do this in an automated way - so that every new model or folder is automatically created in the right schema.</p>\n<p>I was looking at custom schema macros (<a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas\" rel=\"noreferrer\">https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas</a>) but it seems to be plain jinja or simple Python built-ins. Not sure how I would be able to access folder paths in those macros.</p>\n<p>Also, is there a way to write a macro in Python? as it could be relatively straightforward knowing the file path and with the os module.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 11245218,
                "reputation": 2491,
                "user_id": 8248194,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256",
                "display_name": "David Masip",
                "link": "https://stackoverflow.com/users/8248194/david-masip"
            },
            "is_answered": true,
            "view_count": 11566,
            "accepted_answer_id": 62683107,
            "answer_count": 5,
            "score": 15,
            "last_activity_date": 1726659851,
            "creation_date": 1593602271,
            "last_edit_date": 1705011069,
            "question_id": 62675644,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/62675644/write-dbt-test-for-positive-values",
            "title": "Write dbt test for positive values",
            "body": "<p>Is there an easy way to write a test for a column being positive in dbt?</p>\n<p><code>accepted_values</code> doesn't seem to work for continuous variables.</p>\n<p>I know you can write queries in <code>./tests</code> but it looks like an overkill for such a simple thing.</p>\n"
        },
        {
            "tags": [
                "sql",
                "dbt"
            ],
            "owner": {
                "account_id": 16830498,
                "reputation": 394,
                "user_id": 12168607,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mB-3URtt_tRnF6Uo6kddeKtGlBx6JwHJdgyRwzBng=k-s256",
                "display_name": "Shahrukh lodhi",
                "link": "https://stackoverflow.com/users/12168607/shahrukh-lodhi"
            },
            "is_answered": true,
            "view_count": 8151,
            "accepted_answer_id": 75019037,
            "answer_count": 1,
            "score": 15,
            "last_activity_date": 1672924690,
            "creation_date": 1632923231,
            "last_edit_date": 1633087557,
            "question_id": 69377677,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69377677/dbt-how-to-create-custom-table-without-using-ctas",
            "title": "dbt how to create custom table without using CTAS?",
            "body": "<p>I want to create an empty table with specific columns and data types, I don't have any reference table from where I can do SELECT * FROM . The following link has an image which I intend to do <a href=\"https://i.sstatic.net/fmJoo.png\" rel=\"noreferrer\">Please find the attached image</a></p>\n"
        },
        {
            "tags": [
                "sql",
                "postgresql",
                "dbt",
                "sqlfluff"
            ],
            "owner": {
                "account_id": 9960840,
                "reputation": 902,
                "user_id": 7775043,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/iAxC4.png?s=256",
                "display_name": "Albin",
                "link": "https://stackoverflow.com/users/7775043/albin"
            },
            "is_answered": true,
            "view_count": 23590,
            "accepted_answer_id": 66915937,
            "answer_count": 3,
            "score": 14,
            "last_activity_date": 1697444301,
            "creation_date": 1617138133,
            "last_edit_date": 1671287230,
            "question_id": 66878455,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/66878455/configure-sqlfluff-rules",
            "title": "Configure SQLFluff rules",
            "body": "<p>I use SQLFluff to ensure a uniform syntax in the company and reduce error warnings before running the models in dbt. Since our syntax does not completely match the syntax of SQLFluff I would like to make some changes.</p>\n<p>The <a href=\"https://docs.sqlfluff.com/en/stable/rules.html\" rel=\"noreferrer\">Rules References</a> provided by SQLFluff helped me to set up <em>Inline Ignoring Errors</em>, as displayed in the code below (last line of code).</p>\n<p>So I have two questions, which I was not able to answer also with the help of the Rules References of SQLFluff.</p>\n<ol>\n<li><p>I would like to set Rule <em>L032</em> as default 'false' without typing it manually every time in my SQL.</p>\n</li>\n<li><p>How do I change the maximal length of a line regarding Rule <em>L016</em>? I would like to set the default value e.g. 150.</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/byfQX.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/byfQX.png\" alt=\"Rule_L016\" /></a></p>\n<pre><code>SELECT\n    country.country_name,\n    country.population,\n    currency.currency_name,\n    currency.currency_id,\n    currency.strange_long_variable_name_which_is_too_long as not_so_long_variable_name\nFROM country\nLEFT JOIN currency\n    USING (country) -- noqa: L032\n</code></pre>\n<p>I tried to figure it out with the <a href=\"https://docs.sqlfluff.com/en/stable/rules.html\" rel=\"noreferrer\">Rules References</a> but could not figure it out. Help is very much appreciated!</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 18944707,
                "reputation": 233,
                "user_id": 13823885,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/fb3eaba8002b3278e5d5e8a30c9ef52d?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Ravi",
                "link": "https://stackoverflow.com/users/13823885/ravi"
            },
            "is_answered": true,
            "view_count": 57749,
            "answer_count": 5,
            "score": 13,
            "last_activity_date": 1665482976,
            "creation_date": 1593961248,
            "last_edit_date": 1593975106,
            "question_id": 62742369,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/62742369/while-running-the-dbt-run-command-gets-error",
            "title": "while running the dbt run command gets error",
            "body": "<p>connection to dbt and snowfalke was successful but when tried to run this command:</p>\n<pre><code>$ dbt run\n</code></pre>\n<p>it gives this error</p>\n<blockquote>\n<p>ERROR: Runtime Error\nCould not find profile named 'learn_dbt'\nEncountered an error:\nRuntime Error\nCould not run dbt&quot;</p>\n</blockquote>\n<p>Am I making any command mistake?</p>\n"
        },
        {
            "tags": [
                "environment-variables",
                "dbt"
            ],
            "owner": {
                "account_id": 3294496,
                "reputation": 205,
                "user_id": 2772056,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/5c7f1eb211d07507008cb591e0f338f4?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "user2772056",
                "link": "https://stackoverflow.com/users/2772056/user2772056"
            },
            "is_answered": true,
            "view_count": 42775,
            "answer_count": 1,
            "score": 12,
            "last_activity_date": 1721156507,
            "creation_date": 1657646161,
            "last_edit_date": 1663208221,
            "question_id": 72956095,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72956095/dbt-environment-variables-and-running-dbt",
            "title": "DBT - environment variables and running dbt",
            "body": "<p>I am relatively new to DBT and I have been reading about <code>env_var</code> and I want to use this in a couple of situations and I am having difficultly and looking for some support.</p>\n<p>firstly I am trying to use it in my profiles.yml file to replace the user and password so that this can be set when it is invoked. When trying to test this locally (before implementing this on our AWS side) I am failing to find the right syntax and not finding anything useful online.\nI have tried variations of:</p>\n<pre><code>dbt run --vars '{DBT_USER: my_username, DBT_PASSWORD=my_password}'\n</code></pre>\n<p>but it is not recognizing and giving nothing useful error wise. When running dbt run by itself it does ask for <code>DBT_USER</code> so it is expecting it, but doesn't detail how</p>\n<p>I would also like to use it in my <code>dbt_project.yml</code> for the schema but I assume that this will be similar to the above, just a third variable at the end. Is that the case?</p>\n<p>Thanks</p>\n"
        },
        {
            "tags": [
                "python-3.x",
                "windows",
                "cmd",
                "powershell-2.0",
                "dbt"
            ],
            "owner": {
                "account_id": 5333471,
                "reputation": 591,
                "user_id": 4253365,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/wvSzZ.jpg?s=256",
                "display_name": "Ali Hasan",
                "link": "https://stackoverflow.com/users/4253365/ali-hasan"
            },
            "is_answered": true,
            "view_count": 64008,
            "answer_count": 11,
            "score": 12,
            "last_activity_date": 1711621134,
            "creation_date": 1575282296,
            "last_edit_date": 1595411127,
            "question_id": 59136919,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/59136919/installed-dbt-but-getting-error-dbt-command-not-found-error",
            "title": "Installed dbt but getting error &quot;DBT command not found error&quot;",
            "body": "<p>I would like to use <code>dbt</code> (Data build tool) in one of my project. I am facing hurdle while creating a project or using DBT command. </p>\n\n<p>I have completed the installation process as described on DBT website given here: <a href=\"https://docs.getdbt.com/v0.10/docs/windows\" rel=\"noreferrer\">https://docs.getdbt.com/v0.10/docs/windows</a>. DBT installed successfully but when I tried to use DBT command for creating project it gave me error: </p>\n\n<blockquote>\n  <p>'dbt' is not recognized as an internal or external command, operable\n  program or batch file.</p>\n</blockquote>\n\n<p>I am using windows 10, and I have tried it for python 3.6 as well as python 3.7 version.</p>\n\n<p>any help would be highly appreciated!\nThanks</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 2425955,
                "reputation": 2880,
                "user_id": 2117872,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/QjcJX.jpg?s=256",
                "display_name": "David Garrison",
                "link": "https://stackoverflow.com/users/2117872/david-garrison"
            },
            "is_answered": true,
            "view_count": 12672,
            "accepted_answer_id": 72344192,
            "answer_count": 2,
            "score": 12,
            "last_activity_date": 1705840767,
            "creation_date": 1653077869,
            "last_edit_date": 1653078545,
            "question_id": 72324405,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72324405/using-multiple-columns-in-a-unique-key-for-incremental-loading-in-dbt",
            "title": "Using multiple columns in a Unique_Key for incremental loading in DBT",
            "body": "<p>For incremental models, the DBT documentation <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models\" rel=\"noreferrer\">here</a> says:</p>\n<blockquote>\n<p>The unique_key should be supplied in your model definition as a string representing a simple column or a list of single quoted column names that can be used together, for example, ['col1', 'col2', \u2026])</p>\n</blockquote>\n<p>I've built an incremental model in DBT with this incremental definition</p>\n<pre><code>{{\n  config(\n    materialized='incremental',\n    unique_key = ['Col1', 'Col2', 'Col3']\n  )\n}}\n</code></pre>\n<p>Which compiles into this merge statement in in Snowflake:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>using DW_DEV.dbt_dgarrison_DATA_STAGING.MY_TABLE__dbt_tmp as DBT_INTERNAL_SOURCE\n    on \n        DBT_INTERNAL_SOURCE.['Col1', 'Col2', 'Col3'] = DBT_INTERNAL_DEST.['Col1', 'Col2', 'Col3']\n...\n</code></pre>\n<p>And this reasonably throws a SQL ERROR complaining about the brackets:</p>\n<blockquote>\n<p>SQL compilation error: syntax error line 4 at position 32 unexpected '['. syntax error line 4 at position 45 unexpected ','. syntax error line 4 at position 98 unexpected '['. syntax error line 4 at position 111 unexpected ','.</p>\n</blockquote>\n<p>I can't find any other good examples using multiple columns this way. (there are options involving concatenating columns, and I'm open to recommendations on the best approach to that, but I'm trying to figure out how to use the DBT recommended syntax)</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 19534633,
                "reputation": 191,
                "user_id": 14293665,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/71407596d8492a4bb1a630b1aaa3ce4b?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Sweety",
                "link": "https://stackoverflow.com/users/14293665/sweety"
            },
            "is_answered": true,
            "view_count": 31815,
            "answer_count": 4,
            "score": 11,
            "last_activity_date": 1677673031,
            "creation_date": 1600768336,
            "last_edit_date": 1677412122,
            "question_id": 64007239,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/64007239/how-do-we-define-select-statement-as-a-variable-in-dbt",
            "title": "How do we define select statement as a variable in dbt?",
            "body": "<p>Hi I am trying to define a select statement in a set variable in dbt,\ncan any one suggest how to set sql query as a variable in dbt  and how to access those variables in below CTEs?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 9002381,
                "reputation": 1405,
                "user_id": 6710525,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/4oLy6.jpg?s=256",
                "display_name": "totooooo",
                "link": "https://stackoverflow.com/users/6710525/totooooo"
            },
            "is_answered": true,
            "view_count": 6256,
            "accepted_answer_id": 60567491,
            "answer_count": 1,
            "score": 11,
            "last_activity_date": 1624027277,
            "creation_date": 1583503549,
            "question_id": 60565688,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/60565688/what-is-the-user-yml-file-what-is-its-purpose",
            "title": "What is the .user.yml file? What is its purpose?",
            "body": "<p>I'm starting with DBT. When I ran my project it created a <code>.user.yml</code> file. Its content:</p>\n\n<pre><code>{id: c8e8abd2-09a3-4699-b444-3ef7ee5b04e5}\n</code></pre>\n\n<p>It seems from <a href=\"https://github.com/fishtown-analytics/dbt/issues/1645\" rel=\"noreferrer\">this github issue</a> that it's some kind of cookie, but I could not find any info anywhere on what its role is.</p>\n\n<p>Can someone explain the purpose of this file? Should I add it to my <code>.gitignore</code>?</p>\n"
        },
        {
            "tags": [
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 85788,
                "reputation": 1232,
                "user_id": 238968,
                "user_type": "registered",
                "accept_rate": 73,
                "profile_image": "https://www.gravatar.com/avatar/2634b26733c03daf1532b78b84e2be3b?s=256&d=identicon&r=PG",
                "display_name": "Ravi",
                "link": "https://stackoverflow.com/users/238968/ravi"
            },
            "is_answered": true,
            "view_count": 43745,
            "accepted_answer_id": 65617106,
            "answer_count": 1,
            "score": 10,
            "last_activity_date": 1610040206,
            "creation_date": 1610029246,
            "last_edit_date": 1610029468,
            "question_id": 65614108,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/65614108/using-if-block-in-dbt-models",
            "title": "Using if block in dbt models",
            "body": "<p>Apologies for asking dumb question. But I tried many different approaches but none of them seems to work.</p>\n<p>I have a requirement to select data from 2 different tables based on the variable. I am trying to do that in dbt models with if statement but it doesn't seem to work.</p>\n<p>Model looks something like thins:</p>\n<pre><code>SELECT \n*\nFROM\n{% if enable_whitelisting == 'true' %}\n    {{ ref('accounts_whitelisted') }}    accounts\n{% else %}\n        {{ ref('accounts') }}   accounts\n{% endif %}\n</code></pre>\n<p>Any help is appreciated.</p>\n<p>Thanks in advance.</p>\n"
        },
        {
            "tags": [
                "python",
                "dbt"
            ],
            "owner": {
                "account_id": 21871,
                "reputation": 12177,
                "user_id": 53491,
                "user_type": "registered",
                "accept_rate": 56,
                "profile_image": "https://i.sstatic.net/Y3f3O.jpg?s=256",
                "display_name": "Brian Postow",
                "link": "https://stackoverflow.com/users/53491/brian-postow"
            },
            "is_answered": true,
            "view_count": 17754,
            "accepted_answer_id": 75112535,
            "answer_count": 1,
            "score": 10,
            "last_activity_date": 1727499852,
            "creation_date": 1673623710,
            "last_edit_date": 1682610130,
            "question_id": 75111217,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/75111217/how-do-i-run-dbt-models-from-a-python-script-or-program",
            "title": "How do I run DBT models from a Python script or program?",
            "body": "<p>I have a DBT project, and a python script will be grabbing data from the postgresql to produce output.</p>\n<p>However, part of the python script will need to make the DBT run. I haven't found the library that will let me cause a DBT run from an external script, but I'm pretty sure it exists. How do I do this?</p>\n<p>ETA: The correct answer may be to download the DBT CLI and then use python system calls to use that.... I was hoping for a library, but I'll take what I can get.</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 2344016,
                "reputation": 103,
                "user_id": 2054735,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ca6bb72d7a1ac7f012c62cb98e0177ca?s=256&d=identicon&r=PG",
                "display_name": "stuartcoggins",
                "link": "https://stackoverflow.com/users/2054735/stuartcoggins"
            },
            "is_answered": true,
            "view_count": 6203,
            "accepted_answer_id": 69484724,
            "answer_count": 2,
            "score": 10,
            "last_activity_date": 1639417350,
            "creation_date": 1633620526,
            "last_edit_date": 1633620987,
            "question_id": 69483842,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69483842/why-do-i-get-a-select-active-warehouse-error-in-dbt-when-trying-the-table-mate",
            "title": "Why do I get a &#39;select active warehouse&#39; error in dbt when trying the table materialization, but not with the view materialization?",
            "body": "<p>I've been working with dbt for a couple of months now, so still fairly new to it. When running a test model, I have no problems when using the view materialization:</p>\n<pre><code>{{ config(materialized='view') }}\n\nselect 1 as id\n</code></pre>\n<p>Resulting in:</p>\n<pre><code>15:30:25 | 1 of 1 START view model dbt.stg_CampaignTableTest.................... [RUN]\n15:30:26 | 1 of 1 OK created view model dbt.stg_CampaignTableTest............... [SUCCESS 1 in 1.48s]\n</code></pre>\n<p>However, when I make the switch to a table materialization I get an error message about not having an active warehouse selected in Snowflake:</p>\n<pre><code>{{ config(materialized='table') }}\n\nselect 1 as id\n</code></pre>\n<p>Resulting in:</p>\n<pre><code>15:32:52 | 1 of 1 START table model dbt.stg_CampaignTableTest................... [RUN]\n15:32:53 | 1 of 1 ERROR creating table model dbt.stg_CampaignTableTest.......... [ERROR in 1.22s]\n\nDatabase Error in model stg_CampaignTableTest (models/test/stg_CampaignTableTest.sql)\n  000606 (57P03): No active warehouse selected in the current session.  Select an active warehouse with the 'use warehouse' command.\n</code></pre>\n<p>Of course, it's not possible to include a &quot;use warehouse&quot; statement within my test model as it is inserted into the compiled SQL at the wrong position:</p>\n<pre><code>{{ config(materialized='table') }}\n\nuse warehouse &quot;AnalysisTeam_WH&quot;;\n\nselect 1 as id\n</code></pre>\n<p>Because it leads to:</p>\n<pre><code>2021-10-07T15:33:59.366279Z: On model.my_new_project.stg_CampaignTableTest: /* {&quot;app&quot;: &quot;dbt&quot;, &quot;dbt_version&quot;: &quot;0.21.0&quot;, &quot;profile_name&quot;: &quot;user&quot;, &quot;target_name&quot;: &quot;default&quot;, &quot;node_id&quot;: &quot;model.my_new_project.stg_CampaignTableTest&quot;} */\n\n\n      create or replace transient table &quot;AnalysisTeam&quot;.&quot;dbt&quot;.&quot;stg_CampaignTableTest&quot;  as\n      (\n\nuse warehouse &quot;AnalysisTeam_WH&quot;;\n2021-10-07T15:33:59.366342Z: Opening a new connection, currently in state closed\n2021-10-07T15:34:00.163673Z: Snowflake query id: 019f7386-3200-ec67-0000-464100e189fa\n2021-10-07T15:34:00.163803Z: Snowflake error: 001003 (42000): SQL compilation error:\nsyntax error line 4 at position 0 unexpected 'use'.\n</code></pre>\n<p>I appear to have the correct permissions with my Snowflake 'role' to create tables, views, etc., so I was at a loss to understand why changing from view to table would cause the model to fail. I suspect it could be related to Snowflake permissions rather than a dbt issue but I am not sure. Any ideas would be really appreciated!</p>\n<p>Edit: I appeared to make a mistake with my screenshots so I have switched to code snippets which is hopefully clearer.</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 26289587,
                "reputation": 101,
                "user_id": 19956796,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/b85751dab32676d3d656e02175323030?s=256&d=identicon&r=PG",
                "display_name": "paul1315",
                "link": "https://stackoverflow.com/users/19956796/paul1315"
            },
            "is_answered": true,
            "view_count": 2530,
            "answer_count": 1,
            "score": 10,
            "last_activity_date": 1696699879,
            "creation_date": 1662718421,
            "question_id": 73660554,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73660554/can-dbt-cloud-be-setup-to-use-mfa-when-connecting-to-snowflake",
            "title": "Can DBT Cloud be setup to use MFA when connecting to Snowflake?",
            "body": "<p>I have a Snowflake account that uses MFA.\nLogging in to Snowflake promtps for the MFA and I receive a push notification on my phone. Approving that logs me in.\nWhen setting up DBT Cloud it asks for details of the Snowflake account and then tries to test this. The cursor changes to the red circle / line and stays like that for a while before going back to normal and allowing the test to be run again. The Continue option remains greyed out.\nI expect the connection is timing out because Snowflake is waiting on the MFA to be approved, but as there is no notification in Duo there is nothing to approve.\nDBT docs have details of how to setup MFA for CLI but I can't see anything for DBT Cloud.\nIs MFA supported in DBT Cloud connections to Snowflake or should I just have a special Snowflake user that doesn't require MFA?</p>\n"
        },
        {
            "tags": [
                "dbt",
                "confluence"
            ],
            "owner": {
                "account_id": 21400147,
                "reputation": 111,
                "user_id": 15762465,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/3dc822d5c70b0cc7467fad1d2499838c?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "rakimo",
                "link": "https://stackoverflow.com/users/15762465/rakimo"
            },
            "is_answered": true,
            "view_count": 1703,
            "answer_count": 1,
            "score": 10,
            "last_activity_date": 1711136732,
            "creation_date": 1695538402,
            "question_id": 77166007,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/77166007/is-there-a-way-to-embed-dbt-docs-within-confluence",
            "title": "Is there a way to embed dbt docs within Confluence?",
            "body": "<p>dbt's built in docs is amazing for tracking data lineage, data dictionaries, business rules etc. But publishing these docs to a central place in a secure way without using dbt Cloud is proving to be not as straight forward.</p>\n<p>Like a lot of businesses we use Confluence for project tracking and documentation. It would be beneficial to have all documentation centralised in one place for both technical and business users. Has anyone embeded dbt docs within Confluence before? Are there any existing plug-ins for this?</p>\n<p>The nirvana state would be to have some CI which automatically keeps the embeded dbt docs up to date, but baby steps.</p>\n<p>I found this on the Confluence support page which talks about using Confluence to serve static content:\n<a href=\"https://confluence.atlassian.com/confkb/how-to-use-confluence-to-serve-static-content-677282407.html\" rel=\"noreferrer\">https://confluence.atlassian.com/confkb/how-to-use-confluence-to-serve-static-content-677282407.html</a></p>\n<p>But I'd prefer to not muck around with backend Confluence server things. It's a bit outside of my area of expertise.</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 19801591,
                "reputation": 113,
                "user_id": 14502473,
                "user_type": "registered",
                "profile_image": "https://lh5.googleusercontent.com/-w6sAaksslHE/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckQBLTiOR4lRsUtIrvUWCIJ0RLVkQ/s96-c/photo.jpg?sz=256",
                "display_name": "Kyle Cheung",
                "link": "https://stackoverflow.com/users/14502473/kyle-cheung"
            },
            "is_answered": true,
            "view_count": 24544,
            "accepted_answer_id": 64490072,
            "answer_count": 2,
            "score": 9,
            "last_activity_date": 1634713126,
            "creation_date": 1603397149,
            "question_id": 64489772,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/64489772/materialized-view-vs-table-using-dbt",
            "title": "Materialized View vs Table Using dbt",
            "body": "<p>I'm just onboarding dbt and having gone through the tutorial docs I'm wondering if there's a difference between materializing my transformations as views or tables? I'm using Snowflake as the data warehouse. There's some documentation <a href=\"https://docs.snowflake.com/en/user-guide/views-materialized.html#comparison-with-tables-regular-views-and-cached-results\" rel=\"noreferrer\">here</a> that shows the differences between a table and a materialized view but if I'm using dbt to update the tables regularly, do they more or less become the same thing?</p>\n<p>Thanks!</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 23928594,
                "reputation": 91,
                "user_id": 17923414,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/49d42baf9efb76ec3c8bb90c41b3e1b8?s=256&d=identicon&r=PG",
                "display_name": "fmbcooney",
                "link": "https://stackoverflow.com/users/17923414/fmbcooney"
            },
            "is_answered": true,
            "view_count": 15849,
            "answer_count": 4,
            "score": 9,
            "last_activity_date": 1662109987,
            "creation_date": 1642070148,
            "question_id": 70695142,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70695142/dbt-warning-did-not-find-matching-node-for-patch",
            "title": "DBT - [WARNING]: Did not find matching node for patch",
            "body": "<p>I keep getting the error below when I use <em>dbt run</em> - I can't find anything on why this error occurs or how to fix it within the dbt documentation.</p>\n<pre><code>[WARNING]: Did not find matching node for patch with name 'vGenericView' in the 'models' section of file 'models\\generic_schema\\schema.sql'\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 17236657,
                "reputation": 213,
                "user_id": 12479987,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/VEJoa.jpg?s=256",
                "display_name": "Mincho",
                "link": "https://stackoverflow.com/users/12479987/mincho"
            },
            "is_answered": true,
            "view_count": 14847,
            "accepted_answer_id": 65395870,
            "answer_count": 2,
            "score": 9,
            "last_activity_date": 1732542843,
            "creation_date": 1608564032,
            "question_id": 65395382,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/65395382/how-to-run-multi-tag-selector",
            "title": "How to run multi-tag selector",
            "body": "<p>I am using dbt 0.18.1 and I follow the documentation about tags however I am curious to know how to run multi-tag selector as arguments.\nAccording to this:\n<a href=\"https://github.com/fishtown-analytics/dbt/pull/1014\" rel=\"noreferrer\">https://github.com/fishtown-analytics/dbt/pull/1014</a></p>\n<blockquote>\n<p>Select using a mix of tags, fqns, and parent/child selectors:\n$ dbt run --model tag:nightly+ salesforce.*+</p>\n</blockquote>\n<p>Unfortunately this is not really a &quot;mix of tags&quot;. <br/><br/>\nI have tags of [mixpanel_tests, quality] and I wish to run models that have both tags included (not separated). If I run <code>dbt run -m tag:quality -t blabla</code></p>\n<ol>\n<li>I would have executed all models that have QUALITY in the array of tags regardless if its single argument or multiple argument however I wish to run ONLY quality marked. How to do that?</li>\n<li>How do I specify 2 tags or 3 tags selector to run models with the mentioned tags (i.e mixpanel_tests, quality - but only those models that have both tags defined). More or less an AND clause rather than an OR clause.\n<br/>\nHmm I hope it is clear. How to have multitag selector that executes only the combination of tags given?</li>\n</ol>\n"
        },
        {
            "tags": [
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 14897420,
                "reputation": 381,
                "user_id": 10757281,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/10215776085348685/picture?type=large",
                "display_name": "Reid Williams",
                "link": "https://stackoverflow.com/users/10757281/reid-williams"
            },
            "is_answered": true,
            "view_count": 10864,
            "answer_count": 2,
            "score": 9,
            "last_activity_date": 1704212248,
            "creation_date": 1630954445,
            "last_edit_date": 1704212248,
            "question_id": 69079158,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69079158/can-dbt-macros-accept-other-macros-as-arguments",
            "title": "Can dbt macros accept other macros as arguments?",
            "body": "<p>I am curious if I can pass macros into another macro like this:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>{% macro my_macro(a, b, another_macro) %}\n  ...\n  {{ another_macro(a,b) }}\n  ...\n{% endmacro %}\n</code></pre>\n<p><strong>BONUS:</strong>\nIf dbt's framework can allow it am able to how can I pass arguments to it?</p>\n<p>In R it would look like</p>\n<pre><code>my_callable_function &lt;- function(another_function, ...) {\n  another_function(...)\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 17864589,
                "reputation": 890,
                "user_id": 12977233,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/uT6rh.jpg?s=256",
                "display_name": "Shabari nath k",
                "link": "https://stackoverflow.com/users/12977233/shabari-nath-k"
            },
            "is_answered": true,
            "view_count": 6963,
            "answer_count": 2,
            "score": 9,
            "last_activity_date": 1676864117,
            "creation_date": 1654715308,
            "question_id": 72550991,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72550991/how-is-the-usage-of-dbt-better-than-using-stored-procedure-method",
            "title": "How is the usage of DBT better than using stored procedure method?",
            "body": "<p>I have been using <code>stored procedure</code> method for a long time now.</p>\n<p>As a part of sales report generation, I create <code>stored procedures</code> to <code>join/union all</code> mulitple tables in database  and call it using <code>python</code> whenever i need it.</p>\n<p>Now <code>DBT</code> is a hot topic these days.</p>\n<p>Whats the advantage of moving to <code>DBT</code> from <code>stored procedures</code> ?\nIs there any point in migrating my entire <code>stored procedure</code> stack from stored proc to <code>DBT</code>?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 7766729,
                "reputation": 1111,
                "user_id": 5877690,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/bf9430fe081fbefd6a40465fef651ce8?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Simon",
                "link": "https://stackoverflow.com/users/5877690/simon"
            },
            "is_answered": true,
            "view_count": 2174,
            "accepted_answer_id": 70383575,
            "answer_count": 1,
            "score": 9,
            "last_activity_date": 1639687109,
            "creation_date": 1639674610,
            "question_id": 70382990,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70382990/configuring-raw-and-analytics-databases-with-dbt",
            "title": "Configuring raw and analytics databases with dbt",
            "body": "<p>I have been reading dbt's <a href=\"https://blog.getdbt.com/how-we-configure-snowflake/\" rel=\"noreferrer\">How we configure Snowflake</a> guide which explains the rationale behind having a <code>raw</code> database and an <code>analytics</code> database. Raw data is loaded into your warehouse into <code>raw</code> (e.g. by using Fivetran) and <code>analytics</code> is used by dbt to save transformed data/views for data analysts/scientists.</p>\n<p>However, I can't seem to find any guides on how to actually set this up. The profiles.yml file needs to point to where the raw data is, so that dbt can begin transforming. However, this file also seems to dictate the database and schema into which transformed data/views are saved.</p>\n<p>Where in dbt's many .yml files do I specify globally where to save transformed data?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 8495759,
                "reputation": 285,
                "user_id": 6370402,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/405d8de362cab93c0f8c40dfa1d95f96?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Catazza",
                "link": "https://stackoverflow.com/users/6370402/catazza"
            },
            "is_answered": true,
            "view_count": 11340,
            "answer_count": 4,
            "score": 9,
            "last_activity_date": 1668793255,
            "creation_date": 1638789319,
            "question_id": 70244607,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70244607/overriding-source-macro-in-dbt-to-allow-for-dynamic-sources-for-test-runs",
            "title": "Overriding source macro in DBT to allow for dynamic sources for test runs",
            "body": "<p>My aim is to be able to have &quot;dynamic&quot; sources depending on the type of DBT run I am doing. To be more precise, I am trying to find a solution to perform end-to-end business testing of our DBT models. I don't mean schema or simple data tests, but business logic tests. Something like, I have some input tables with test data, I run the DBT models and then I assert the end tables contain the desired results. I can create all the target tables in a separate schema by using a different 'test' profile, but I still need to be able to select from a different set of sources, which would be the test tables I am creating with the test data.</p>\n<p>I guess I can use jinja in the source files in combination with some variables to make this happen, but I am wondering if there is an even better way where I can do it without changing the source files at all. Like, the developers would not have to worry about writing code that also works for the tests. For this purpose, I was wondering if we can override the source macro, or do something along these lines, to incorporate this behaviour - similar to when we override the <code>generate_schema_name</code> macro. Something along the lines of (in python pseudocode):</p>\n<pre><code>def source(schema_name, table_name):\n    if env('is_test') == true:\n        return schema_name + table_name + '_test'\n    else:\n        return schema_name + table_name\n</code></pre>\n<p>I guess the complexity here is also that the source macro does more than that, for example sets some info for the lineage for the docs, which I definitely would like to keep.</p>\n<p>Any suggestion outside of this method are more than welcome!</p>\n"
        },
        {
            "tags": [
                "python",
                "database",
                "gitlab",
                "dbt"
            ],
            "owner": {
                "account_id": 21188503,
                "reputation": 139,
                "user_id": 15581402,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ee09bcfdfc113b3e38b754ed61bd9df3?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "zafar",
                "link": "https://stackoverflow.com/users/15581402/zafar"
            },
            "is_answered": true,
            "view_count": 11594,
            "answer_count": 1,
            "score": 9,
            "last_activity_date": 1617907594,
            "creation_date": 1617872919,
            "question_id": 67000794,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67000794/how-can-we-work-with-multiple-dbt-projects",
            "title": "How can we work with multiple DBT projects",
            "body": "<p>can anyone share how to organise multiple projects in dbt , given the best practices. My present  abstract architecture hierarchy is as follows</p>\n<pre><code>Analytics \n--.dbt/\n-----profiles.yml\n--projects/\n-----project_1/\n----------models/\n----------dbt_project.yml\n-----project_2/\n----------models/\n----------dbt_project.yml\n--tests/\n-----projects/\n----------project_1/\n----------project_2/\n</code></pre>\n<p>To the point of creating models for either projects_1 or project_2 is working perfectly fine.</p>\n<p>but the problem comes when i try to run the test (model unit tests) for project_2.when I run it gives the error\n' no dbt_project.yml found at expected path in temp/project_1/ ...  ' (the unit tests that i am trying to run is for project_2)\nhowever, the paths are absolutely correct but the lookup for dbt_project.yml is in wrong directory (in the temp directory). On the side note, some of the project_2 models does depends on few project_1 models.</p>\n<p>can anyone share or reference where i can get the help to solve this sort of multi dbt project issue.</p>\n"
        },
        {
            "tags": [
                "snowflake-task",
                "dbt"
            ],
            "owner": {
                "account_id": 18944707,
                "reputation": 233,
                "user_id": 13823885,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/fb3eaba8002b3278e5d5e8a30c9ef52d?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Ravi",
                "link": "https://stackoverflow.com/users/13823885/ravi"
            },
            "is_answered": true,
            "view_count": 15563,
            "answer_count": 5,
            "score": 8,
            "last_activity_date": 1695757621,
            "creation_date": 1593762402,
            "last_edit_date": 1593808231,
            "question_id": 62710809,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/62710809/dbt-to-snowflake-connections-fails-via-profiles-yml",
            "title": "dbt to snowflake connections fails via profiles.yml",
            "body": "<p>I'm trying to connect to snowflake via dbt but connections fail with the error below:</p>\n<pre><code>Using profiles.yml file at /home/myname/.dbt/profiles.yml\nUsing dbt_project.yml file at /mnt/c/Users/Public/learn_dbt/rks-learn-dbt/learn_dbt/dbt_project.yml\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\nProfile loading failed for the following reason:\nRuntime Error\n  Could not find profile named 'learn_dbt'\nRequired dependencies:\n - git [OK found] \n</code></pre>\n<p>Any advice please.</p>\n<p>Note: I am learning to setup dbt connections looking at udemy videos.</p>\n<p>Below is my <code>profiles.yml</code> file:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>learn_dbt:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: XXXXXX\n      user: XXXX                \n      password: XXXX                     \n      role: transform_role\n      database: analytics\n      warehouse: transform_wh\n      schema: dbt\n      threads: 1\n      client_session_keep_alive: False\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 19121362,
                "reputation": 101,
                "user_id": 13965268,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GivskPRmZS6iPki9mC2CB2NjopK-23cP1_paSPGqQ=k-s256",
                "display_name": "Niks A",
                "link": "https://stackoverflow.com/users/13965268/niks-a"
            },
            "is_answered": true,
            "view_count": 13550,
            "answer_count": 3,
            "score": 8,
            "last_activity_date": 1595521139,
            "creation_date": 1595270501,
            "last_edit_date": 1595486921,
            "question_id": 63001872,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63001872/will-dbt-support-temp-table-creation-like-create-table-temp1-as-select-from-t",
            "title": "will DBT support temp table creation like create table #temp1 as select * from tab1 or it works only CTE way",
            "body": "<p>I found out a way to handle the temp tables in DBT, write all those in pre-hook and call the final temp table in the outside of the pre-hook, tested and is working fine, able to reduce the code running time from more than 20 mins to 1 min. But I see one problem that we can't see the lineage graph in the DBT documents.\nIs there any way to handle the temp tables other than pre-hook and with lineage in Docs?</p>\n"
        },
        {
            "tags": [
                "docker",
                "airflow",
                "dbt"
            ],
            "owner": {
                "account_id": 15622610,
                "reputation": 2306,
                "user_id": 11271048,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/8thDR.jpg?s=256",
                "display_name": "alt-f4",
                "link": "https://stackoverflow.com/users/11271048/alt-f4"
            },
            "is_answered": true,
            "view_count": 4048,
            "accepted_answer_id": 65465102,
            "answer_count": 1,
            "score": 8,
            "last_activity_date": 1609071215,
            "creation_date": 1609064540,
            "last_edit_date": 1609071215,
            "question_id": 65464756,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/65464756/running-dbt-within-airflow-through-the-docker-operator",
            "title": "Running DBT within Airflow through the Docker Operator",
            "body": "<p>Building my question on <a href=\"https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo\">How to run DBT in airflow without copying our repo</a>, I am currently running airflow and syncing the dags via git. I am considering different option to include DBT within my workflow. One suggestion by <a href=\"https://stackoverflow.com/users/3823815/louis-guitton\">louis_guitton</a> is to Dockerize the DBT project, and run it in Airflow via the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\">Docker Operator</a>.</p>\n<p>I have no prior experience using the Docker Operator in Airflow or generally DBT. I am wondering if anyone has tried or can provide some insights about their experience incorporating that workflow, my main questions are:</p>\n<ol>\n<li>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</li>\n<li>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</li>\n<li>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</li>\n</ol>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "airflow",
                "dbt",
                "mwaa"
            ],
            "owner": {
                "account_id": 7563927,
                "reputation": 395,
                "user_id": 8569490,
                "user_type": "registered",
                "accept_rate": 67,
                "profile_image": "https://www.gravatar.com/avatar/8c410c41dfe1d3f94b14736fc8c63d0b?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "nariver1",
                "link": "https://stackoverflow.com/users/8569490/nariver1"
            },
            "is_answered": true,
            "view_count": 6845,
            "answer_count": 4,
            "score": 8,
            "last_activity_date": 1656725573,
            "creation_date": 1622815004,
            "question_id": 67838522,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67838522/how-to-use-dbt-with-aws-managed-airflow",
            "title": "How to use DBT with AWS Managed Airflow?",
            "body": "<p>hope you are doing well.\nI wanted to check if anyone has get up and running with dbt in aws mwaa airflow.</p>\n<p>I have tried without success <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"noreferrer\">this one</a> and <a href=\"https://github.com/tomasfarias/airflow-dbt-python\" rel=\"noreferrer\">this</a> python packages but fails for some reason or another (can't find the dbt path, etc).</p>\n<p>Did anyone has managed to use MWAA (Airflow 2) and DBT without having to build a docker image and placing it somewhere?</p>\n<p>Thank you!</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 5555123,
                "reputation": 5243,
                "user_id": 4406793,
                "user_type": "registered",
                "accept_rate": 78,
                "profile_image": "https://www.gravatar.com/avatar/c1962611febcc643123f5a3049d99f04?s=256&d=identicon&r=PG",
                "display_name": "Marco Roy",
                "link": "https://stackoverflow.com/users/4406793/marco-roy"
            },
            "is_answered": true,
            "view_count": 4784,
            "answer_count": 1,
            "score": 8,
            "last_activity_date": 1664488682,
            "creation_date": 1634681669,
            "last_edit_date": 1634764411,
            "question_id": 69638187,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69638187/invalid-jwt-token-happens-randomly-when-using-snowflakedbt",
            "title": "Invalid JWT token happens randomly when using Snowflake+DBT",
            "body": "<p>We've been using Snowflake+DBT with <a href=\"https://docs.snowflake.com/en/user-guide/key-pair-auth.html\" rel=\"noreferrer\">key pair authentication</a> for a long time now, and we've never had any issues.</p>\n<p>Recently, we started getting random connection errors on some models:</p>\n<pre><code>250001 (08001): Failed to connect to DB: account.region.snowflakecomputing.com:443. JWT token is invalid.\n</code></pre>\n<p>Most of the models will work, but some of them might fail. This can happen to any model, and it's never the same one -- sometimes it's the very first one, sometimes it's the very last one, and sometimes it's a bunch of them. It happens in runs with lots of models, or in runs with a single model.</p>\n<p>It's very inconsistent, and there doesn't seems to be any kind of pattern to it. Sometimes it works, and sometimes it doesn't.</p>\n<p>We've also tried with 1, 4, or 8 threads, and it happens regardless.</p>\n<p>Obviously, there's nothing wrong with the credentials or configurations \u2014 otherwise, nothing would run at all. So I assume there must be something wrong with how DBT is handling the connection(s).</p>\n<p>Interestingly, the errors only happens locally (so far). We haven't seen it in DBT Cloud runs.</p>\n<p>DBT version is 0.20.2 in both cases. We tried with other versions (0.21.0, 0.20.0 &amp; 0.19.1), and the issue persists. I don't know why we're just encountering this, as we've used these other versions previously without any issues.</p>\n<p>It's similar to <a href=\"https://stackoverflow.com/questions/65811588/snowflake-jdbc-driver-throws-net-snowflake-client-jdbc-snowflakesqlexception-jw\">this question</a>, except in our case it doesn't happen consistently at all. We tried connecting &quot;without a region&quot; (using <a href=\"https://docs.snowflake.com/en/user-guide/organizations.html\" rel=\"noreferrer\">Snowflake Organizations</a>), but it doesn't make any difference:</p>\n<pre><code>250001 (08001): Failed to connect to DB: organization-account.snowflakecomputing.com:443. JWT token is invalid.\n</code></pre>\n<p>Is there anything we can do to resolve this?</p>\n<p><em><strong>EDIT:</strong></em> When the error happens, the model hangs for 60 seconds until the error appears.</p>\n<p><em><strong>EDIT 2:</strong></em> I think the error might have started happening when we started using the <a href=\"https://hub.docker.com/r/fishtownanalytics/dbt\" rel=\"noreferrer\">DBT-provided Docker images</a>. Not sure exactly what might be wrong with them, but we'll try going back to our own custom images and see if that works.</p>\n"
        },
        {
            "tags": [
                "postgresql",
                "dbt"
            ],
            "owner": {
                "account_id": 1226044,
                "reputation": 1862,
                "user_id": 1191545,
                "user_type": "registered",
                "accept_rate": 73,
                "profile_image": "https://i.sstatic.net/XQaPc.jpg?s=256",
                "display_name": "Brylie Christopher Oxley",
                "link": "https://stackoverflow.com/users/1191545/brylie-christopher-oxley"
            },
            "is_answered": true,
            "view_count": 12089,
            "accepted_answer_id": 69774042,
            "answer_count": 3,
            "score": 7,
            "last_activity_date": 1635536706,
            "creation_date": 1612521873,
            "question_id": 66061826,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/66061826/add-index-to-dbt-model-column",
            "title": "Add index to dbt model column?",
            "body": "<p>We are considering using dbt to manage models in our PostgreSQL data warehouse. Since dbt models are SQL select statements, there doesn't seem to be an obvious, or documented, way to specify that a particular column should have an index.</p>\n<p>How can we specify column indexes on dbt models?</p>\n"
        },
        {
            "tags": [
                "airflow",
                "dbt"
            ],
            "owner": {
                "account_id": 7315486,
                "reputation": 10163,
                "user_id": 5573294,
                "user_type": "registered",
                "accept_rate": 68,
                "profile_image": "https://i.sstatic.net/f7wtO.jpg?s=256",
                "display_name": "Canovice",
                "link": "https://stackoverflow.com/users/5573294/canovice"
            },
            "is_answered": true,
            "view_count": 4860,
            "accepted_answer_id": 64890419,
            "answer_count": 3,
            "score": 7,
            "last_activity_date": 1686245991,
            "creation_date": 1605691112,
            "last_edit_date": 1686245935,
            "question_id": 64890144,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo",
            "title": "How to run DBT in airflow without copying our repo",
            "body": "<p>We use airflow to orchestrate our workflows, and dbt with bigquery for our daily transformations in BigQuery. We have two separate git repos, one for our dbt project and a separate one for airflow.</p>\n<p>It seems the simplest approach to scheduling our daily <code>run dbt</code> seems to be a <code>BashOperator</code> in airflow. However, to schedule DBT to run with Airflow, it seems like our entire DBT project would need to be nested inside of our Airflow project, that way we can point to it for our <code>dbt run</code> bash command?</p>\n<p>Is it possible to trigger our <code>dbt run</code> and <code>dbt test</code> without moving our DBT directory inside of our Airflow directory? With the <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"nofollow noreferrer\">airflow-dbt package</a>,  for the <code>dir</code> in the <code>default_args</code>, maybe it is possible to point to the gibhub link for the DBT project here?</p>\n"
        },
        {
            "tags": [
                "sql",
                "dbt"
            ],
            "owner": {
                "account_id": 16448714,
                "reputation": 83,
                "user_id": 11882668,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-eLYh2olrS3c/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdfDB4DS2ZnGhYUhDdcPCdLDQq0Vg/photo.jpg?sz=256",
                "display_name": "Nripesh Pradhan",
                "link": "https://stackoverflow.com/users/11882668/nripesh-pradhan"
            },
            "is_answered": true,
            "view_count": 9990,
            "accepted_answer_id": 69445792,
            "answer_count": 3,
            "score": 7,
            "last_activity_date": 1714127147,
            "creation_date": 1633369871,
            "question_id": 69440332,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69440332/how-do-i-loop-through-alll-columns-using-jinja-in-dbt",
            "title": "How do I loop through alll columns using Jinja in DBT?",
            "body": "<p>I want to iterate over all the columns using dbt.</p>\n"
        },
        {
            "tags": [
                "snapshot",
                "dbt"
            ],
            "owner": {
                "account_id": 25554789,
                "reputation": 161,
                "user_id": 19417549,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AATXAJzc9iXylA8X89UJr6s4yZwmm70XGNtgPMdmnf5h=k-s256",
                "display_name": "Cade Justad-Sandberg",
                "link": "https://stackoverflow.com/users/19417549/cade-justad-sandberg"
            },
            "is_answered": true,
            "view_count": 3649,
            "accepted_answer_id": 72817843,
            "answer_count": 1,
            "score": 7,
            "last_activity_date": 1656600537,
            "creation_date": 1656210664,
            "last_edit_date": 1656210709,
            "question_id": 72758509,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72758509/encountered-unknown-tag-snapshot-in-dbt-snapshot",
            "title": "Encountered unknown tag &#39;snapshot&#39; in DBT Snapshot",
            "body": "<p>I want to run a DBT Snapshot and am following a near-identical template to the one outlined in the <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/snapshots#snapshot-configurations\" rel=\"noreferrer\">documentation</a>. However, I get the error when I run <code>dbt snpashot</code></p>\n<pre><code>Compilation Error in model test_snapshot (.../project_folder/snapshots/test_snapshot.sql)    \nEncountered unknown tag 'snapshot'.\n        line 1\n          {% snapshot test_snapshot %}\n</code></pre>\n<p>Below is the code I am attempting to compile.</p>\n<pre><code>{% snapshot test_snapshot %}\n    {{\n        config(\n            strategy='check',\n            unique_key='id',\n            target_schema='snapshots',\n            check_cols= 'all'\n        )\n    }}\n\nselect\n        *\nfrom {{ ref('modle_in_sample_folder') }}\n\n{% endsnapshot %}\n</code></pre>\n<p>The order of the snapshot folder and ref file is .../project_folder/snapshots/test_snapshot.sql and .../project_folder/intermediate/model_in_sample_folder.sql</p>\n"
        },
        {
            "tags": [
                "sql",
                "dbt"
            ],
            "owner": {
                "account_id": 5310584,
                "reputation": 1604,
                "user_id": 4237080,
                "user_type": "registered",
                "accept_rate": 75,
                "profile_image": "https://graph.facebook.com/1210377595/picture?type=large",
                "display_name": "brienna",
                "link": "https://stackoverflow.com/users/4237080/brienna"
            },
            "is_answered": true,
            "view_count": 18549,
            "accepted_answer_id": 72736650,
            "answer_count": 1,
            "score": 7,
            "last_activity_date": 1665422361,
            "creation_date": 1655990277,
            "question_id": 72730868,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72730868/get-column-names-and-types-using-star-macro-in-dbt",
            "title": "Get column names AND types using star macro in dbt",
            "body": "<p>Using the star macro, is there a way to also get the column data type (boolean, numerical, etc), in addition to the column name?</p>\n<p>For example, this query uses the star macro to collect the column names from a reference table, saves it as an array variable <code>column_names</code>, and then I loop over this array and apply the max function to all the columns.</p>\n<pre><code>{% set column_names = star(\n    from=ref_table,\n    except=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n    as_list=True)\n%}\n\nselect \n    date_trunc('week', day) as week,\n    name,\n\n    {%- for col in column_names %}  \n    max({{ col|lower }}) as {{ col | lower }}{%- if not loop.last %},{{ '\\n  ' }}{% endif %}\n    {%- endfor %}\n\nfrom {{ ref('my_table_name') }}    \ngroup by 1, 2\n</code></pre>\n<p>I would like to conditionally apply the max function to only boolean columns.</p>\n<p>This might look something like</p>\n<pre><code>{%- for col in column_names %}  \n    {% if is_boolean(col) %}  \n    max({{ col|lower }}) as {{ col | lower }}{%- if not loop.last %},{{ '\\n  ' }}{% endif %}\n    {% endif %}\n{%- endfor %}\n</code></pre>\n<p>but the problem is that the star macro passes the column names as a string, so it's not carrying any metadata with it.</p>\n<p>How might I get the column data type here?</p>\n<p>Data warehouse: Snowflake</p>\n"
        },
        {
            "tags": [
                "apache-spark",
                "hive",
                "parquet",
                "dbt"
            ],
            "owner": {
                "account_id": 3910809,
                "reputation": 1848,
                "user_id": 3236215,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://i.sstatic.net/wzVbH.jpg?s=256",
                "display_name": "IVR",
                "link": "https://stackoverflow.com/users/3236215/ivr"
            },
            "is_answered": true,
            "view_count": 8689,
            "accepted_answer_id": 63493307,
            "answer_count": 2,
            "score": 7,
            "last_activity_date": 1597951706,
            "creation_date": 1597852355,
            "last_edit_date": 1597906759,
            "question_id": 63490730,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63490730/using-external-parquet-tables-in-a-dbt-pipeline",
            "title": "Using external parquet tables in a DBT pipeline",
            "body": "<p>I'm trying to set up a simple DBT pipeline that uses a parquet tables stored on Azure Data Lake Storage and creates another tables that is also going to be stored in the same location.</p>\n<p>Under my <code>models/</code> (which is defined as my sources path) I have 2 files <code>datalake.yml</code> and <code>orders.sql</code>. <code>datalake.yml</code> looks like this:</p>\n<pre><code>version:2\nsources:\n   - name: datalake\n     tables:\n        - name: customers\n          external:\n             location: path/to/storage1 # I got this by from file properties in Azure\n             file_format: parquet\n          columns:\n             - name: id\n               data_type: int\n               description: &quot;ID&quot;\n             - name: ...\n</code></pre>\n<p>My <code>orders.sql</code> table looks like this:</p>\n<pre><code>{{config(materialized='table', file_format='parquet', location_root='path/to/storage2')}}\nselect name, age from {{ source('datalake', 'customers') }}\n</code></pre>\n<p>I'm also using the <code>dbt-external-tables</code> package. Also note that when I run <code>dbt debug</code> everything is fine and I can connect to my database (which happens to be Databricks).</p>\n<p>I tried running <code>dbt run-operation stage_external_sources</code> which returns <code>Error: staging external sources is not implemented for the default adapter</code>. When I run <code>dbt run</code>, I get <code>Error: UnresolvedRelation datalake.customers</code>.</p>\n<p>Or perhaps I could make use of the hive metastore instead somehow? Any tips on how I could fix this would be highly appreciated!</p>\n"
        },
        {
            "tags": [
                "dbt",
                "meltano"
            ],
            "owner": {
                "account_id": 4187455,
                "reputation": 2571,
                "user_id": 4298208,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/598481447/picture?type=large",
                "display_name": "aaronsteers",
                "link": "https://stackoverflow.com/users/4298208/aaronsteers"
            },
            "is_answered": true,
            "view_count": 1920,
            "accepted_answer_id": 70383829,
            "answer_count": 1,
            "score": 7,
            "last_activity_date": 1640716446,
            "creation_date": 1639675242,
            "question_id": 70383118,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70383118/using-dbt-and-meltano-how-can-i-prevent-multiple-dbt-job-runs-from-conflicting",
            "title": "Using dbt and Meltano, how can I prevent multiple dbt job runs from conflicting with each other?",
            "body": "<p>When running dbt jobs in Meltano, <code>dbt run</code> jobs may collide with each other if run out of a triggered context - for instance, when an on-demand job collides with a scheduled job or a CI-based job.</p>\n<p>If <code>dbt run</code> operates on the same tables at the same time, this generally causes a crash and sometimes a data quality issue if the same insert is performed twice on a single target table.</p>\n<p>Any way to prevent run collisions, using either Meltano functionality or native dbt functionality?</p>\n"
        },
        {
            "tags": [
                "python",
                "dbt"
            ],
            "owner": {
                "account_id": 1414093,
                "reputation": 3432,
                "user_id": 1340097,
                "user_type": "registered",
                "accept_rate": 70,
                "profile_image": "https://www.gravatar.com/avatar/5d9279085a5dc48b6f9ee14c4bacc0f7?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "s_curry_s",
                "link": "https://stackoverflow.com/users/1340097/s-curry-s"
            },
            "is_answered": true,
            "view_count": 8844,
            "accepted_answer_id": 70071661,
            "answer_count": 2,
            "score": 7,
            "last_activity_date": 1723563378,
            "creation_date": 1636570759,
            "last_edit_date": 1636571321,
            "question_id": 69918818,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69918818/how-can-i-run-python-code-after-a-dbt-run-or-a-specific-model-is-completed",
            "title": "How can I run python code after a DBT run (or a specific model) is completed?",
            "body": "<p>I would like to be able to run an ad-hoc python script that would access and run analytics on the model calculated by a dbt run, are there any best practices around this?</p>\n"
        },
        {
            "tags": [
                "readability",
                "dbt",
                "schema.yml"
            ],
            "owner": {
                "account_id": 9431343,
                "reputation": 103,
                "user_id": 7014797,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/10154746549250967/picture?type=large",
                "display_name": "Kelly Danielle",
                "link": "https://stackoverflow.com/users/7014797/kelly-danielle"
            },
            "is_answered": true,
            "view_count": 2801,
            "accepted_answer_id": 69561724,
            "answer_count": 1,
            "score": 7,
            "last_activity_date": 1673848954,
            "creation_date": 1634150262,
            "question_id": 69560670,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69560670/dbt-breaking-up-schema-yml-file",
            "title": "DBT breaking up schema.yml file",
            "body": "<p>We have a large schema.yml file in our DBT folders. It is not the cleanest or easiest to find what we need in it. I am curious if anyone knows of a way to split up this file. I am not trying to overcomplicate things and separate the dbt project into multiple or anything like that but rather just work on cleaning up the schema.yml file for readability etc. Thanks!</p>\n"
        },
        {
            "tags": [
                "sql",
                "google-bigquery",
                "dbt"
            ],
            "owner": {
                "account_id": 6284827,
                "reputation": 123,
                "user_id": 5335584,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/SzUwN.jpg?s=256",
                "display_name": "Edgar",
                "link": "https://stackoverflow.com/users/5335584/edgar"
            },
            "is_answered": true,
            "view_count": 4058,
            "accepted_answer_id": 66319143,
            "answer_count": 3,
            "score": 7,
            "last_activity_date": 1669202278,
            "creation_date": 1613735882,
            "question_id": 66277165,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/66277165/how-to-choose-the-latest-partition-of-a-bigquery-table-in-dbt-without-scanning-t",
            "title": "How to choose the latest partition of a bigquery table in DBT without scanning the whole table?",
            "body": "<p>I'm trying to select the latest partition from a BigQuery table without scanning the whole table in a DBT model in order to save query costs.</p>\n<p>DBT doesnt allow using semicolons in a data model so using the <code>DECLARE</code>+<code>SET</code> scripting statements doesn't work as suggested <a href=\"https://stackoverflow.com/questions/39733826/how-to-choose-the-latest-partition-in-bigquery-table\">here</a>.</p>\n<p>DBT has a sql_header macro which allows setting some variables in the header but that header doesn't accept references to a data model or at least the following code is not compiling:</p>\n<pre><code>{{ config(\n  sql_header=&quot;  DECLARE latest_partition_date DATE;\n  DECLARE latest_load_timestamp TIMESTAMP;\n  SET latest_partition_date = (SELECT MAX(_PARTITIONDATE) FROM {{ ref(&quot;model&quot;) }} );\n  SET latest_load_timestamp = (SELECT MAX(loaded_at) FROM {{ ref(&quot;model&quot;) }} WHERE _PARTITIONDATE = latest_partition_date);&quot;\n) }}\n\n-- set the main query\nSELECT * FROM {{ ref(&quot;model&quot;) }}\nWHERE \n-- Select the latest partition to reduce 'Bytes processed' for loading the query.\n_PARTITIONDATE = latest_partition_date\n-- Select the latest load within the latest partition to get only one duplicate of data.\nAND loaded_at = latest_load_timestamp\n</code></pre>\n<p>I need to solve this in standard SQL.</p>\n<p>Other methods that were suggested included setting <code>WHERE _PARTITIONDATE = CURRENT_DATE()</code> or using <code>DATE_SUB(CURRENT_DATE(), 3)</code> but those don't satisfy because data load breakages are unpredictable and only dynamically selecting the latest would work here. Is that possible?</p>\n"
        },
        {
            "tags": [
                "sql-server",
                "dbt"
            ],
            "owner": {
                "account_id": 19534633,
                "reputation": 191,
                "user_id": 14293665,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/71407596d8492a4bb1a630b1aaa3ce4b?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Sweety",
                "link": "https://stackoverflow.com/users/14293665/sweety"
            },
            "is_answered": true,
            "view_count": 9141,
            "answer_count": 2,
            "score": 7,
            "last_activity_date": 1645940345,
            "creation_date": 1602073024,
            "question_id": 64243858,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/64243858/can-we-convert-delete-sql-statements-into-dbt",
            "title": "can we convert delete SQL statements into DBT?",
            "body": "<p>I am trying to build a DBT model from SQL which has delete statements based on where clause.</p>\n<p>Can any one please suggest me how to convert the below SQL delete statement into DBT model?</p>\n<p>'''\ndelete table_name where condition;</p>\n<p>'''</p>\n<p>Thanks</p>\n"
        },
        {
            "tags": [
                "databricks",
                "dbt",
                "databricks-unity-catalog"
            ],
            "owner": {
                "account_id": 26212897,
                "reputation": 145,
                "user_id": 19891453,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AItbvmmik0TBJOsok-O4iHA8IJhF4-GuEQj6WD9zv_sH=k-s256",
                "display_name": "Ashley Betts",
                "link": "https://stackoverflow.com/users/19891453/ashley-betts"
            },
            "is_answered": false,
            "view_count": 3116,
            "answer_count": 2,
            "score": 7,
            "last_activity_date": 1666735200,
            "creation_date": 1663023904,
            "last_edit_date": 1663787566,
            "question_id": 73696068,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73696068/dbt-on-databricks-unity-catalog",
            "title": "DBT on Databricks Unity Catalog",
            "body": "<p>I've been considering turning on Databricks Unity Catalog in our primary (only) workspace, but I'm concerned about how this might impact our existing dbt loads with the new three-part object references.</p>\n<p>I see from the dbt-databricks <a href=\"https://pypi.org/project/dbt-databricks/\" rel=\"nofollow noreferrer\">release notes</a> that you need &gt;= 1.1.1 to get unity support. <br>The snippet with it only shows setting the <em>catalog</em> property in the profile. I was planning on having some of the sources in separate catalog's for the dbt generated objects.</p>\n<p>I might even choose to have the dbt generated objects in separate catalogues if this was available.<br>\nAs turning on Unity Catalog is a one way road in a workspace, I don't wish to wing it and see what happens.</p>\n<p>Has anyone used dbt with Unity Catalog and used numerous catalogs in the project?</p>\n<p>If so, are there any gotcha's and how do you specify the catalog for sources and specific models?</p>\n<p>Regards,</p>\n<p>Ashley</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 22034448,
                "reputation": 141,
                "user_id": 16301202,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/89b605c54762a4e7b6ec5fee728ca89a?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Mark",
                "link": "https://stackoverflow.com/users/16301202/mark"
            },
            "is_answered": true,
            "view_count": 6423,
            "accepted_answer_id": 69713706,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1635190134,
            "creation_date": 1635186728,
            "last_edit_date": 1635190134,
            "question_id": 69713087,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69713087/dbt-relationship-test-compilation-error-test-definition-dictionary-must-have-ex",
            "title": "dbt relationship test compilation error: test definition dictionary must have exactly one key",
            "body": "<p>I'm a new user of dbt, trying to write a relationship test:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>- name: PROTOCOL_ID\n  tests:\n    - relationships:\n    to: ref('Animal_Protocols')\n    field: id\n</code></pre>\n<p>I am getting this error:</p>\n<pre><code>Compilation Error\nInvalid test config given in models/Animal_Protocols/schema.yml:\ntest definition dictionary must have exactly one key, got [('relationships', None), ('to', &quot;ref('Animal_Protocols')&quot;), ('field', 'id')] instead (3 keys)\n@: UnparsedNodeUpdate(original_file_path='model...ne)\n</code></pre>\n<p>&quot;unique&quot; and &quot;not-null&quot; tests in the same file are working fine, but I have a similar error with &quot;accepted_values&quot;.</p>\n<p>I am using dbt cli version 0.21.0 with Snowflake on MacOS Big Sur 11.6.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 10601951,
                "reputation": 73,
                "user_id": 7808631,
                "user_type": "registered",
                "profile_image": "https://lh5.googleusercontent.com/-q4UBGwxebGc/AAAAAAAAAAI/AAAAAAAAECk/2KuU4MufEK0/photo.jpg?sz=256",
                "display_name": "Chetan Grandhe",
                "link": "https://stackoverflow.com/users/7808631/chetan-grandhe"
            },
            "is_answered": true,
            "view_count": 9653,
            "answer_count": 3,
            "score": 6,
            "last_activity_date": 1668417420,
            "creation_date": 1647975890,
            "question_id": 71577610,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71577610/dbt-doesnt-throw-an-error-when-running-dbt-docs-generate-but-the-catalog-json-i",
            "title": "DBT doesn&#39;t throw an error when running dbt docs generate but the catalog.json is missing",
            "body": "<p>I'm new to dbt, I am successfully able to create my models and schemas and macros, but when I do dbt docs generate I get manifest.json, but not catalog.json and so <code>dbt docs serve</code> is failing. It throws the below error.</p>\n<p><a href=\"https://i.sstatic.net/WPPrB.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/WPPrB.png\" alt=\"enter image description here\" /></a></p>\n<p>I checked through my logs I don't find an error, I ran the generated sql from logs in snowflake and it works as well.</p>\n<p>Below is what I get when I run dbt docs generate</p>\n<pre><code>(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; dbt docs generate\n18:45:10  Running with dbt=1.0.4\n18:45:10  Found 9 models, 4 tests, 0 snapshots, 0 analyses, 180 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n18:45:10\n18:45:15  Concurrency: 1 threads (target='dev')\n18:45:15\n18:45:16  Done.\n18:45:16  Building catalog\n(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; \n\n</code></pre>\n<p>Is there a way to understand where this issue is coming from? Is it possible to better debug dbt docs generate command?</p>\n<p>Thank you,\nSai.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 13032007,
                "reputation": 2400,
                "user_id": 9418115,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/6YyIP.png?s=256",
                "display_name": "Adrien Arcuri",
                "link": "https://stackoverflow.com/users/9418115/adrien-arcuri"
            },
            "is_answered": true,
            "view_count": 29133,
            "answer_count": 3,
            "score": 6,
            "last_activity_date": 1726639193,
            "creation_date": 1638965052,
            "question_id": 70274718,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70274718/how-to-declare-and-init-variable-in-a-dbt-model-in-sql-file-with-big-query-ad",
            "title": "How to declare and init variable in a dbt model in `.sql` file with big query adaptor?",
            "body": "<p>I would like to declare and init a variable in a dbt model <code>customer.sql</code> file.\nI used the keyword <code>DECLARE</code> to declare a variable like the <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting\" rel=\"noreferrer\">BigQuery documentation</a> suggests but I got\na <code>Syntax error</code> on <code>DECLARE</code> keyword.</p>\n<p>Code:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>DECLARE myDate VARCHAR DEFAULT '2021-01-01';\n\nwith order_bis as (\n\n    select\n        order_id\n\n    from\n        order\n    where\n        customer_date &gt; myDate\n\n)\n\nselect * from order_bis\n</code></pre>\n<p>Error:\n<code>Syntax error: Expected &quot;(&quot; or keyword SELECT or keyword WITH but got keyword DECLARE ...</code></p>\n"
        },
        {
            "tags": [
                "compiler-errors",
                "yaml",
                "dbt"
            ],
            "owner": {
                "account_id": 19477684,
                "reputation": 101,
                "user_id": 14248767,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/1893494615b4da67e197309052aa9967?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Andresbi",
                "link": "https://stackoverflow.com/users/14248767/andresbi"
            },
            "is_answered": true,
            "view_count": 19275,
            "accepted_answer_id": 68447240,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1626730190,
            "creation_date": 1626694697,
            "question_id": 68439855,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/68439855/in-dbt-when-i-add-a-well-formatted-yml-file-to-my-project-i-stop-being-able-t",
            "title": "In dbt, when I add a well formatted .yml file to my project, I stop being able to run dbt or compile SQL until I delete the yaml file. Why?",
            "body": "<p>Thank you in advance for helping me in my journey! I am a dbt newb, doing the <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17813360-learning-objectives\" rel=\"noreferrer\">dbt fundamentals course</a>. I am following the directions exactly.</p>\n<p>Sequence of the issue:</p>\n<ol>\n<li><p>I created .sql files in my models folder and subfolders, compiled them, ran dbt, and they showed up in my Snowflake DW. No problem</p>\n</li>\n<li><p>I added a .yml file to one of the sub folders\n<a href=\"https://i.sstatic.net/QLfPl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QLfPl.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>The issue happens when I click &quot;save&quot; to save the code to the .yml file - that the course provided <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17704336-practice\" rel=\"noreferrer\">here</a>. (See compilation error in screenshot)</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/MCVAF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/MCVAF.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"4\">\n<li>When I click on Compilation Error, I get this (Refreshing the IDE does not help):</li>\n</ol>\n<p>Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.\n'version'</p>\n<p><a href=\"https://i.sstatic.net/Les2W.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Les2W.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"5\">\n<li><p>At this point, I am not able to run any of the models, even those in other subfolders. For example\n<a href=\"https://i.sstatic.net/5sxOS.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/5sxOS.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>Also, those .sql files also have the same compilation error above.</p>\n</li>\n<li><p>When I delete the .yml file, everything goes to normal and all the errors disappear.</p>\n</li>\n<li><p>things I have tried:</p>\n</li>\n</ol>\n<ul>\n<li>Deleting all the contents of the yml file and hitting save --&gt; the compilation error goes away</li>\n<li>Changing indentation</li>\n<li>Only leaving a single test instead of two</li>\n<li>Creating the yml file in a different subfolder</li>\n<li>Signing out of dbt and back in</li>\n</ul>\n<p>Please help!\nthank you</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 17236657,
                "reputation": 213,
                "user_id": 12479987,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/VEJoa.jpg?s=256",
                "display_name": "Mincho",
                "link": "https://stackoverflow.com/users/12479987/mincho"
            },
            "is_answered": true,
            "view_count": 14496,
            "accepted_answer_id": 62996073,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1601374231,
            "creation_date": 1595236781,
            "last_edit_date": 1601374231,
            "question_id": 62992304,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/62992304/querying-with-dbt-from-external-source",
            "title": "Querying with dbt from external source",
            "body": "<p>I have the following issue:</p>\n<ul>\n<li>I have an AWS S3 pipeline that on a daily basis a single json.gz files is spit.</li>\n<li>I wish to take that file with dbt and put it into snowflake (no snowpipe use atm)</li>\n</ul>\n<p>I have managed to do this by creating a storage integration and I have manually created with my role (used for running dbt) a schema and assing usage on that schema. So far so good.</p>\n<p>Then I read about this:</p>\n<p><a href=\"https://github.com/fishtown-analytics/dbt-external-tables\" rel=\"nofollow noreferrer\">https://github.com/fishtown-analytics/dbt-external-tables</a></p>\n<p>Problem is that this is the only way this runs properly, I had to alter my dbt profiles.yml, set the default schema to be S3_MIXPANEL with default database RAW_DEV, run a different target and role on that with --target 'ingest_dev' parameter.</p>\n<p>I keep thinking that there should be a more sophisticated solution, where I can create schema's and query metadata and use something like {{ source() }} so I can point my documentation somehow that this is an external source. This dbt-external-tables is not really well explained for my case here I think?</p>\n<p>Please can anyone help me and share how to create schemas and query from external stages properly without changing default schema macro &amp;  dbtprofiles.yml each time?</p>\n<p>I have succeeded to run the following code:</p>\n<pre><code>{{\n  config(\n    materialized ='incremental',\n    schema = generate_schema_name('S3_MIXPANEL')\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd') as event_date,\n    $1 as payload,\n    CONVERT_TIMEZONE('Europe/London',TO_TIMESTAMP_tz($1:properties:mp_processing_time_ms::int / 1000)) as  event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as ingested_at\n\n from\n\n    @my_s3_stage\n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date&gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }} \n</code></pre>\n<p>EDIT 22-06-20:</p>\n<p>I have added the src_mixpanel.yml file in my models and ran the dbt command, however I had to also specify the data_types, so I added them too, then I apparently had to add the &quot;macro&quot; in my macros too (btw maybe a stupid question but I don't really know how to install your package, so I manually added all macros from yours into mine).</p>\n<p>Now when I run this code:</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>with</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: RAW_DEV\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n              data_type: date\n            - name: file_name\n              expression: metadata$filename\n              data_type: string\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>I get an error:</p>\n<blockquote>\n<p>Encountered an error while running operation: Compilation Error in macro stage_external_sources (macros/stage_external_sources.sql)<br />\n'dict object' has no attribute 'sources'</p>\n</blockquote>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 1012277,
                "reputation": 21204,
                "user_id": 1024586,
                "user_type": "registered",
                "accept_rate": 87,
                "profile_image": "https://www.gravatar.com/avatar/3238fd4bbeafdce226ff3406e859f2d6?s=256&d=identicon&r=PG",
                "display_name": "Doug Fir",
                "link": "https://stackoverflow.com/users/1024586/doug-fir"
            },
            "is_answered": true,
            "view_count": 9822,
            "accepted_answer_id": 75607081,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1677691698,
            "creation_date": 1677653544,
            "question_id": 75600390,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/75600390/exclude-multiple-models-from-run",
            "title": "Exclude multiple models from run",
            "body": "<p>When running dbt with descendants, I would like to exclude two models. I can exclude one model like so:</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts\n</code></pre>\n<p>The above works as expected.</p>\n<p>I tried the following to exclude multiple models.</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts,growth_scorecard\n</code></pre>\n<p>In this case neither nz_daily_cohorts nor growth_scorecard were excluded.</p>\n<p>Then tried:</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts --exclude growth_scorecard\n</code></pre>\n<p>Again, in this case neither nz_daily_cohorts nor growth_scorecard were excluded.</p>\n<p>How can I run <code>dbt run ga4_update_set+</code> but also exclude both <code>nz_daily_cohorts</code> and <code>growth_scorecard</code>?</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 23603389,
                "reputation": 101,
                "user_id": 17639491,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/0dd4676de94d74fcc8eac050345fbeb1?s=256&d=identicon&r=PG",
                "display_name": "NavySeal2026",
                "link": "https://stackoverflow.com/users/17639491/navyseal2026"
            },
            "is_answered": true,
            "view_count": 11697,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1680516146,
            "creation_date": 1674146529,
            "question_id": 75175545,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/75175545/setting-dbt-date-variable",
            "title": "Setting dbt date variable",
            "body": "<p>I am trying to set a date variable within a dbt model to be the date 7 days ago. The model will run against a Redshift database. I have done the following to set the variable, however I get the error DATE_ADD is not defined:</p>\n<p><code>{%- set start_date = TRUNC(DATE_ADD(day, -7, CURRENT_DATE)) -%}</code></p>\n<p>What is the correct way to set the variable.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 26212897,
                "reputation": 145,
                "user_id": 19891453,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AItbvmmik0TBJOsok-O4iHA8IJhF4-GuEQj6WD9zv_sH=k-s256",
                "display_name": "Ashley Betts",
                "link": "https://stackoverflow.com/users/19891453/ashley-betts"
            },
            "is_answered": true,
            "view_count": 7499,
            "accepted_answer_id": 73589026,
            "answer_count": 2,
            "score": 6,
            "last_activity_date": 1711343203,
            "creation_date": 1661992822,
            "question_id": 73563223,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73563223/dbt-conditionally-set-schema-config",
            "title": "DBT: conditionally set schema config",
            "body": "<p>I'm trying to determine how I can conditionally set schema config attributes. I've attempted this by a macro in both dbt_project.yml and also in schema.yml but both of these methods fail with:</p>\n<pre><code>00:23:19  Encountered an error:\nCompilation Error\n  Could not render {{get_location_root('lndv')}}: 'get_location_root' is undefined\n</code></pre>\n<p>The outcome I would like to achieve is conditionally setting <em>location_root</em> for Spark for various schemas. I want different locations for each environment. I thought the macro path was the best fit as this follows a pattern but it obviously doesn't work in <em>dbt_project.yml</em> or property files. I was using <em>target.name</em> to determine environment. It's in the same directory as other macros that are successfully rendering in models so the path is set correctly. I don't really want to resort to placing this config in each model if I can avoid it.</p>\n<p>Does anyone have any thoughts on how I can solve this? Either getting the macro to work in <em>dbt_project.yml</em> / <em>schema.yml</em> or by some other method?</p>\n<p>Regards,</p>\n<p>Ashley</p>\n"
        },
        {
            "tags": [
                "python",
                "sql",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 19745346,
                "reputation": 83,
                "user_id": 14457920,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/sc5dd.jpg?s=256",
                "display_name": "Gus",
                "link": "https://stackoverflow.com/users/14457920/gus"
            },
            "is_answered": true,
            "view_count": 22409,
            "answer_count": 2,
            "score": 6,
            "last_activity_date": 1623737250,
            "creation_date": 1623688239,
            "question_id": 67973996,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67973996/how-do-i-loop-through-nested-structs-using-jinja-in-dbt",
            "title": "How do I loop through nested structs using Jinja in DBT?",
            "body": "<p>I'm trying to build a model in DBT that flattens out a struct with name <code>properties</code> that contains about a hundred structs inside it (e.g. <code>property1</code>, <code>property2</code>, etc.), each with 5 different columns of which I want to extract one called <code>value</code>. I could type <code>properties.propertyX.value</code> 100 times, but I figured I could try to find a way to loop through each struct within <code>properties</code> and obtain <code>propertyX.value</code> within a <code>SELECT</code> statement by using Jinja, but I guess I'm either unfamiliar with the syntax or its limitations because I don't know what to do. I've tried:</p>\n<pre><code>WITH t as (\n    SELECT\n        properties\n    FROM\n        src\n)\nSELECT\n    {% for property in properties %}\n    {{property}}.value\n    {% endfor %}\n    {%- if not loop.last %},{% endif -%}\nFROM\n    t\n</code></pre>\n<p>but I realized I have to set <code>properties</code> as a variable and I don't really know how to do that in a way that it references the individual properties in the <code>properties</code> struct. Anyway, I'm quite lost and if someone could help I would be so grateful.</p>\n"
        },
        {
            "tags": [
                "python",
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 1740883,
                "reputation": 1095,
                "user_id": 1592334,
                "user_type": "registered",
                "accept_rate": 62,
                "profile_image": "https://www.gravatar.com/avatar/172b0cf44146a9046f6814cea0351806?s=256&d=identicon&r=PG",
                "display_name": "Abiodun Adeoye",
                "link": "https://stackoverflow.com/users/1592334/abiodun-adeoye"
            },
            "is_answered": true,
            "view_count": 5637,
            "accepted_answer_id": 74032477,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1674205079,
            "creation_date": 1665506357,
            "question_id": 74031424,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74031424/how-to-implement-python-udf-in-dbt",
            "title": "How to implement Python UDF in dbt",
            "body": "<p>Please I need some help with applying python UDF to run on my dbt models.\nI successfully created a python function in snowflake (DWH) and ran it against a table. This seems to work as expected, but implementing this on dbt seems to be a struggle. Some advice/help/direction will make my day.</p>\n<p>here is my python UDF created on snowflake</p>\n<pre><code>create or replace function &quot;077&quot;.&quot;Unity&quot;.sha3_512(str varchar)\nreturns varchar\nlanguage python\nruntime_version = '3.8'\nhandler = 'hash'\nas\n\n$$\nimport hashlib\n \ndef hash(str):\n    # create a sha3 hash object\n    hash_sha3_512 = hashlib.new(&quot;sha3_512&quot;, str.encode())\n\n    return hash_sha3_512.hexdigest()\n$$\n;\n</code></pre>\n<p>The objective is the create the python function in dbt and apply it to the model below</p>\n<pre><code>{{ config(materialized = 'view') }}\n\nWITH SEC AS(\n    SELECT \n         A.&quot;AccountID&quot; AS AccountID,\n         A.&quot;AccountName&quot; AS AccountName , \n         A.&quot;Password&quot; AS Passwords,\n apply function here (A.&quot;Password&quot;) As SHash\n    FROM {{ ref('Green', 'Account') }} A\n   )\n\n----------------VIEW RECORD------------------------------ \n\nSELECT * \nFROM SEC\n</code></pre>\n<p>is there a way to do this please. Thank you</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 21888305,
                "reputation": 61,
                "user_id": 16175677,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxJ8-UiLro2Iv9i8rdYoaC3wwi3M-IJUtYU6sK_=k-s256",
                "display_name": "Ilhomjon Oripov",
                "link": "https://stackoverflow.com/users/16175677/ilhomjon-oripov"
            },
            "is_answered": true,
            "view_count": 25217,
            "answer_count": 4,
            "score": 6,
            "last_activity_date": 1672129109,
            "creation_date": 1623241733,
            "last_edit_date": 1645669358,
            "question_id": 67904146,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67904146/dbt-depends-on-a-source-not-found",
            "title": "dbt depends on a source not found",
            "body": "<p>Could you please help me with this issue?</p>\n<pre><code>Encountered an error:\nCompilation Error in model metrics_model (models\\example\\metrics_model.sql)\n  Model 'model.test_project.metrics_model' (models\\example\\metrics_model.sql) depends on a source named 'automate.metrics' which was not found\n</code></pre>\n<p>I am having this monotonous error, which I have not been able to solve.</p>\n<p>Many thanks beforehand!</p>\n"
        },
        {
            "tags": [
                "macos",
                "yaml",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 26951930,
                "reputation": 65,
                "user_id": 20521017,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/ALm5wu1NHJHWRmp8G6RLxs2M7oD2se0XzcYnVT3DE-W4=k-s256",
                "display_name": "TheLoneGrape",
                "link": "https://stackoverflow.com/users/20521017/thelonegrape"
            },
            "is_answered": true,
            "view_count": 10377,
            "accepted_answer_id": 74864260,
            "answer_count": 2,
            "score": 6,
            "last_activity_date": 1737476924,
            "creation_date": 1671500514,
            "last_edit_date": 1671500579,
            "question_id": 74857734,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74857734/with-dbt-how-do-i-use-a-jinja-macro-in-a-yaml-file",
            "title": "With dbt. How do I use a jinja macro in a yaml file",
            "body": "<p>I have a yaml file like this:</p>\n<pre><code>models:\n       - name: test_view\n        description: &quot;test&quot;\n        config:\n          meta:\n            database_tags:\n              ACCOUNT_OBJECTS.TAGS.ENV: DEV`\n\n\n</code></pre>\n<p>I am trying automatically change 'DEV' to PROD when it's in that environment. I have a macro that gets the variable from targets.name</p>\n<p>This is the jinja code:</p>\n<pre><code>{% macro test_macro(target) %}\n      {%- if  target.name == &quot;dev&quot; -%} DEV\n      {%- elif target.name == &quot;prod&quot;  -%} PROD\n      {%- else -%} invalid\n      {%- endif -%}\n    {% endmacro %}`\n</code></pre>\n<p>However, when I try to use the macro I get 'test_macro is undefined'</p>\n<p>eg. ACCOUNT_OBJECTS.TAGS.ENV: {{ test_macro(target)}}</p>\n<p>Is it that custom macros still cannot be used in yaml files?</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 15780469,
                "reputation": 422,
                "user_id": 11387016,
                "user_type": "registered",
                "profile_image": "https://lh6.googleusercontent.com/-HtaIyJwBGZo/AAAAAAAAAAI/AAAAAAAAGFA/OGcg1-mtDWo/photo.jpg?sz=256",
                "display_name": "ShayHa",
                "link": "https://stackoverflow.com/users/11387016/shayha"
            },
            "is_answered": true,
            "view_count": 10379,
            "accepted_answer_id": 70414249,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1640010000,
            "creation_date": 1639831570,
            "last_edit_date": 1639832557,
            "question_id": 70403550,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70403550/automatically-add-new-column-to-incremental-or-other-type",
            "title": "Automatically add new column to incremental (or other type)",
            "body": "<p>I need some wisdom for a new  DBT use case that I am trying to solve. I am pretty new to DBT and not sure what is the most efficient DBT way. We are using snowflake as our DWH.</p>\n<h3>Problem</h3>\n<p>We have a lot of incremental models that are managed with DBT. Lately, we had the need to add a new column to all models. What would be the most efficient DBT way to do it? Should we override the incremental macro script? (<a href=\"https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/include/snowflake/macros/materializations/incremental.sql\" rel=\"noreferrer\">I found this for snowflake</a>.) I assume that the last resort will be to add the new column manually to each model.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 6677719,
                "reputation": 854,
                "user_id": 5151861,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/DNhNg.jpg?s=256",
                "display_name": "iamtodor",
                "link": "https://stackoverflow.com/users/5151861/iamtodor"
            },
            "is_answered": true,
            "view_count": 3125,
            "accepted_answer_id": 75094977,
            "answer_count": 3,
            "score": 6,
            "last_activity_date": 1688754456,
            "creation_date": 1659687908,
            "question_id": 73246787,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73246787/dbt-check-what-packages-are-installed",
            "title": "dbt: check what packages are installed",
            "body": "<p>Is there a way to check what packages are installed? I would expect something like <code>dbt list packages</code>?</p>\n<p>The context is:</p>\n<ul>\n<li>Until I run <code>dbt deps</code> the content of <code>packages.yml</code> gives me nothing. And there are some situations when the models could be triggered without running <code>dbt deps</code></li>\n<li>I would like to check the packages in runtime</li>\n</ul>\n<p>I searched over google and <code>dbt --help</code> but I didn't find anything.</p>\n"
        },
        {
            "tags": [
                "python",
                "apache-spark",
                "dbt"
            ],
            "owner": {
                "account_id": 20534012,
                "reputation": 211,
                "user_id": 15070559,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/7d9de67ac30e6e0d6dc6520f6588c58b?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "sanchit08",
                "link": "https://stackoverflow.com/users/15070559/sanchit08"
            },
            "is_answered": true,
            "view_count": 2992,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1677197279,
            "creation_date": 1677152946,
            "last_edit_date": 1677153938,
            "question_id": 75544431,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/75544431/how-to-override-global-dbt-macros-in-my-dbt-package-that-will-be-used-by-other-p",
            "title": "How to override global DBT macros in my dbt package that will be used by other projects",
            "body": "<p>I have a DBT package named <code>dbt_helpers</code>, where i intend to override some of dbt's in built global macros. In this example i intend to override the macro <code>dbt_spark_validate_get_file_format</code>, which is present in the dbt spark adapter <a href=\"https://github.com/dbt-labs/dbt-spark/blob/main/dbt/include/spark/macros/adapters.sql\" rel=\"noreferrer\">here</a>.</p>\n<p>I have referred the dbt docs specified <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch\" rel=\"noreferrer\">here</a> to implement my use case. Here is how i have implemented the macro in my package under package's <code>macros</code> folder.</p>\n<pre><code>{% macro dbt_spark_validate_get_file_format(raw_file_format) -%}\n    {{ return(adapter.dispatch('dbt_spark_validate_get_file_format','dbt_helpers')(raw_file_format)) }}\n{%- endmacro %}\n\n\n{% macro default__dbt_spark_validate_get_file_format(raw_file_format) %}\n    {% do log('overriding global macro', info=true) %}\n    {#  Custom implementation here  #}\n    \n{% endmacro %}\n</code></pre>\n<p>I have used the macro namespace <code>dbt_helpers</code> same as my package name. I have specified this in my main DBT project as a package in the <code>packages.yml</code> and I am able to see the macros defined in the <code>dbt_packages</code> directory after running the command <code>dbt deps</code>. In my main dbt project's <code>dbt_project.yml</code> I have included the project level dispatch config to take the macro from my package as shown, as directed in <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch#overriding-global-macros\" rel=\"noreferrer\">this</a> section of the dbt docs.</p>\n<pre><code>dispatch:\n  - macro_namespace: dbt\n    search_order: ['dbt_helpers','dbt']\n</code></pre>\n<p>However when I run my dbt model the macro defined in my package is not being called, rather the inbuilt global macro is still being called. I am able to override the macro by placing it directly inside my projects macros folder, but i need to override the macro from my <code>dbt_helpers</code> package. How can i manage to do this?</p>\n"
        },
        {
            "tags": [
                "sql",
                "database",
                "list",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 14897420,
                "reputation": 381,
                "user_id": 10757281,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/10215776085348685/picture?type=large",
                "display_name": "Reid Williams",
                "link": "https://stackoverflow.com/users/10757281/reid-williams"
            },
            "is_answered": true,
            "view_count": 20677,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1667238833,
            "creation_date": 1597798493,
            "last_edit_date": 1597981192,
            "question_id": 63478564,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63478564/dbt-macro-to-iterate-over-item-in-list-within-a-sql-call",
            "title": "dbt macro to iterate over item in list within a sql call?",
            "body": "<p>First off, I am a dbt backer! I love this tool and the versatility of it.</p>\n<p>When reading some of the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/adapter#drop_relation\" rel=\"noreferrer\">docs</a> I noticed that I might be able to do some meta work on my schemas every time I call a macro.</p>\n<p>One of those would be to clean up schemas.</p>\n<p>(<em>This has been edited as per discussion within the dbt slack</em>)</p>\n<ol>\n<li><p><code>dbt run-operation freeze</code> that would introspect all of the tables that would be written with dbt run but with an autogenerated hash (might just be timestamp). It would output those tables in the schema of my choice and would log the \u201chash\u201d to console.</p>\n</li>\n<li><p><code>dbt run-operation unfreeze --args '{hash: my_hash}'</code> that would then proceed to find the tables written with that hash prefix and clean them out of the schema.</p>\n</li>\n</ol>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 21040750,
                "reputation": 75,
                "user_id": 15463238,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi-bc7esDtde9mqLJb3noaFL2Hl4ASTaylzXMxI=k-s256",
                "display_name": "PolFrx",
                "link": "https://stackoverflow.com/users/15463238/polfrx"
            },
            "is_answered": true,
            "view_count": 23280,
            "accepted_answer_id": 74572646,
            "answer_count": 3,
            "score": 5,
            "last_activity_date": 1700585307,
            "creation_date": 1669224263,
            "question_id": 74550798,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74550798/is-there-a-way-to-force-a-full-refresh-for-a-dbt-incremental-model-if-its-model",
            "title": "Is there a way to force a full refresh for a dbt incremental model if its model changed?",
            "body": "<p>I'll implement dbt for pipelines in Snowflake with incremental models to save query costs but I want to manage the changes of schemas that will be quite frequent. I will have one daily ETL job for each env running a <code>dbt run</code>.\nAlso, in qa and prod environments I'll not be able to run any cmd as I don't have access to these environments for security issues, only to dev.</p>\n<p>Is it possible to trigger a full refresh of a model if its schema changed?</p>\n<p>I saw that we can use the <code>on_schema_change</code> option with incremental models but this will just add (or drop) columns without populating them which is not exactly what I'm looking for as I'll not be able to run a force refresh manually in qa and prod.</p>\n<p>Thanks a lot</p>\n"
        },
        {
            "tags": [
                "google-cloud-platform",
                "google-cloud-composer",
                "dbt"
            ],
            "owner": {
                "account_id": 21808310,
                "reputation": 51,
                "user_id": 16106224,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gh-iwo8e_CxvPl-fXbF5Vr0bhK-O84uTLaLoZNB=k-s256",
                "display_name": "Michel van Dijck",
                "link": "https://stackoverflow.com/users/16106224/michel-van-dijck"
            },
            "is_answered": true,
            "view_count": 3218,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1679456027,
            "creation_date": 1622639996,
            "last_edit_date": 1622703563,
            "question_id": 67805961,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67805961/how-to-set-up-dbt-with-google-cloud-composer",
            "title": "How to set up dbt with Google Cloud Composer?",
            "body": "<p>I am trying to install dbt on Google Cloud Composer but run into dependency issues. I have followed the instructions from this article: <a href=\"https://blog.doit-intl.com/setup-dbt-with-cloud-composer-ab702454e27b\" rel=\"noreferrer\">https://blog.doit-intl.com/setup-dbt-with-cloud-composer-ab702454e27b</a> however at step 2: installing the packages (airflow-dbt &amp; dbt) in composer, it already fails.</p>\n<p>I find the following in the Cloud build logs:</p>\n<pre><code>ERROR: snowflake-connector-python 2.3.6 has requirement boto3&lt;1.16,&gt;=1.4.4, but you'll have boto3 1.17.85 which is incompatible.\nERROR: snowflake-connector-python 2.3.6 has requirement requests&lt;2.24.0, but you'll have requests 2.24.0 which is incompatible.\nERROR: networkx 2.5.1 has requirement decorator&lt;5,&gt;=4.3, but you'll have decorator 5.0.9 which is incompatible.\nERROR: hologram 0.0.13 has requirement jsonschema&lt;3.2,&gt;=3.0, but you'll have jsonschema 3.2.0 which is incompatible.\nERROR: dbt-core 0.19.1 has requirement idna&lt;2.10, but you'll have idna 2.10 which is incompatible.\nERROR: dbt-core 0.19.1 has requirement requests&lt;2.24.0,&gt;=2.18.0, but you'll have requests 2.24.0 which is incompatible.\nERROR: dbt-snowflake 0.19.1 has requirement cryptography&lt;4,&gt;=3.2, but you'll have cryptography 3.0 which is incompatible.\nERROR: dbt-bigquery 0.19.1 has requirement google-api-core&lt;1.24,&gt;=1.16.0, but you'll have google-api-core 1.28.0 which is incompatible.\nERROR: dbt-redshift 0.19.1 has requirement boto3&lt;1.16,&gt;=1.4.4, but you'll have boto3 1.17.85 which is incompatible.\n</code></pre>\n<p>My current environment configuration contains: composer-1.13.0-airflow-1.10.12</p>\n<p>Has anyone encountered the same problem and have you been able to solve it?\nI have also tried to install the specific versions of the requirements listed in the logs but this does not resolve the problem.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 19631558,
                "reputation": 51,
                "user_id": 14369764,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/6a4672a71d863e44815a4ff0f2a91371?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "jayoub",
                "link": "https://stackoverflow.com/users/14369764/jayoub"
            },
            "is_answered": true,
            "view_count": 11252,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1643736388,
            "creation_date": 1601494081,
            "question_id": 64144483,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/64144483/could-not-find-adapter-type-bigquery",
            "title": "Could not find adapter type bigquery",
            "body": "<p>Having trouble running dbt today... encountered this error message and cannot debug the issue. I did not have this issue yesterday and have not changed anything since.</p>\n<p>Installed dbt with Homebrew</p>\n<pre><code>Running with dbt=0.18.0\ndbt version: 0.18.0\n...\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\nProfile loading failed for the following reason:\nRuntime Error\n  Credentials in profile &quot;dandelion-bq&quot;, target &quot;dev&quot; invalid: Runtime Error\n    Could not find adapter type bigquery!\n</code></pre>\n"
        },
        {
            "tags": [
                "aws-lambda",
                "dbt"
            ],
            "owner": {
                "account_id": 11806467,
                "reputation": 135,
                "user_id": 8638818,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/bb796b69c31e5c45c02d2e122ec78980?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "mcanizo",
                "link": "https://stackoverflow.com/users/8638818/mcanizo"
            },
            "is_answered": true,
            "view_count": 5163,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1653414382,
            "creation_date": 1645720171,
            "question_id": 71255224,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71255224/how-to-run-dbt-in-aws-lambda",
            "title": "How to run DBT in AWS Lambda?",
            "body": "<p>I have currently dockerized my DBT solution and I launch it in AWS Fargate (triggered from Airflow). However, Fargate requires about 1 minute to start running (image pull + resource provisioning + etc.), which is great for long running executions (hours), but not for short ones (1-5 minutes).</p>\n<p>I'm trying to run my docker container in AWS Lambda instead of in AWS Fargate for short executions, but I encountered several problems during this migration.</p>\n<p>The one I cannot fix is related to the bellow message, at the time of running the <code>dbt deps --profiles-dir . &amp;&amp; dbt run -t my_target --profiles-dir . --select my_model</code></p>\n<pre><code>Running with dbt=0.21.0\nEncountered an error:\n[Errno 38] Function not implemented\n</code></pre>\n<p>It says there is no function implemented but I cannot see anywhere which is that function. As it appears at the time of installing dbt packages (redshift and dbt_utils), I tried to download them and include them in the docker image (set local paths in <em>packages.yml</em>), but nothing changed. Moreover, DBT writes no logs at this phase (I set the <em>log-path</em> to <em>/tmp</em> in the <em>dbt_project.yml</em> so that it can have write permissions within the Lambda), so I'm blind.</p>\n<p>Digging into this problem, I've found that this can be related to multiprocessing issues within AWS Lamba (my docker image contains python scripts), as stated in <a href=\"https://github.com/dbt-labs/dbt-core/issues/2992\" rel=\"noreferrer\">https://github.com/dbt-labs/dbt-core/issues/2992</a>. I run DBT from python using the <code>subprocess</code> library.</p>\n<p>Since it may be a multiprocessing issue, I have also tried to set <code>&quot;threads&quot;: 1</code> in <em>profiles.yml</em> but it did not solve the problem.</p>\n<p>Does anyone succeeded in deploying DBT in AWS Lambda?</p>\n"
        },
        {
            "tags": [
                "python",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 21393716,
                "reputation": 345,
                "user_id": 17488892,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GirYtT8uPj8AbR06sA04q4pruJIlZ9D_igoeq__Sg=k-s256",
                "display_name": "Aleksandra",
                "link": "https://stackoverflow.com/users/17488892/aleksandra"
            },
            "is_answered": true,
            "view_count": 8040,
            "accepted_answer_id": 73364308,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1662425911,
            "creation_date": 1660579833,
            "question_id": 73363627,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73363627/pass-a-macro-as-a-parameter-jinja-dbt",
            "title": "pass a macro as a parameter jinja dbt",
            "body": "<p>{{ today_date_milliseconds() }} - is my macro in the project. How to redirect this macro as a parameter, so it will be by default and I could in yml write another macro?</p>\n<pre><code>{% test valid_date(model, column_name, exclude_condition = '1=1') %}\n\n    SELECT {{ column_name }}\n    FROM {{ model }}\n    WHERE (CAST( {{ column_name }} AS BIGINT) &lt; {{ today_date_milliseconds() }}\n    AND {{ exclude_condition }}\n\n{% endtest %}\n</code></pre>\n<p>In yml it will look like</p>\n<pre><code>        - name: date_3\n          description: column for third date\n          tests:\n          - valid_date:\n                lower_bound: 'name of another macro'\n\n</code></pre>\n"
        },
        {
            "tags": [
                "web",
                "model",
                "documentation",
                "dbt",
                "fishtown-analytics"
            ],
            "owner": {
                "account_id": 4160423,
                "reputation": 713,
                "user_id": 14899625,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/477d0bd5a135d51df1a7c8842b3d7b4a?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "user961",
                "link": "https://stackoverflow.com/users/14899625/user961"
            },
            "is_answered": true,
            "view_count": 1744,
            "accepted_answer_id": 69192862,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1631707827,
            "creation_date": 1631701938,
            "question_id": 69191415,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69191415/dbt-docs-generate-override-the-default-overview-page-with-custom-content-in-th",
            "title": "DBT docs generate - Override the default overview page with custom content in the documentation website",
            "body": "<p>I am using the dbt docs generate for generating my project's documentation website. I want to override the default overview page with my own custom content in the website. Is it possible to do that?</p>\n"
        },
        {
            "tags": [
                "python",
                "postgresql",
                "dbt"
            ],
            "owner": {
                "account_id": 16130802,
                "reputation": 713,
                "user_id": 11644523,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/2a7d35b9fef8b4c817abf9e35b84f5e2?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Dametime",
                "link": "https://stackoverflow.com/users/11644523/dametime"
            },
            "is_answered": true,
            "view_count": 920,
            "accepted_answer_id": 75022353,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1672940426,
            "creation_date": 1672932325,
            "question_id": 75020740,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/75020740/dbt-postgres-all-models-appending-schema-public-to-output",
            "title": "dbt postgres - all models appending schema public_ to output",
            "body": "<p>I am testing a local setup of dbt-postgres. I have a simple model, but for some reason, any table created is being placed in a schema with the prefix <code>public</code> appended to it.</p>\n<p>Desired output table:\n<code>public.test</code></p>\n<p>Current output table:\n<code>public_public.test</code></p>\n<p>As you can see, the public schema is being duplicated here. Using another schema in the model also creates a new schema with <code>public_</code> prefix.</p>\n<p>Simple model test.sql file:</p>\n<pre><code>{{ config(materialized='table', schema='public') }}\n\nselect a from table x\n</code></pre>\n<p>dbt_profile.yml</p>\n<pre><code>name: 'abc'\nversion: '0.1'\nconfig-version: 2\nprofile: 'abc'\nmodel-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analyses&quot;]\ntest-paths: [&quot;tests&quot;]\nseed-paths: [&quot;seeds&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot; \nclean-targets:       \n  - &quot;target&quot;\n  - &quot;dbt_packages&quot;\n  - &quot;dbt_modules&quot;\n  - &quot;logs&quot;\n\nmodels:\n  abc:\n      materialized: table\n</code></pre>\n<p>profiles.yml</p>\n<pre><code>abc:\n  outputs:\n    dev:\n      type: postgres\n      threads: 1\n      host: &quot;localhost&quot;\n      port: 5432\n      user: &quot;admin&quot;\n      pass: &quot;admin&quot;\n      dbname: database\n      schema: &quot;public&quot;\n  target: dev\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 15787495,
                "reputation": 51,
                "user_id": 11391802,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-psszHVuJp5Q/AAAAAAAAAAI/AAAAAAAAEM0/HdR5myImRvM/photo.jpg?sz=256",
                "display_name": "Manoj Kumar",
                "link": "https://stackoverflow.com/users/11391802/manoj-kumar"
            },
            "is_answered": true,
            "view_count": 6774,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1670844839,
            "creation_date": 1598904523,
            "last_edit_date": 1599226591,
            "question_id": 63677530,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63677530/dbt-cannot-create-two-resources-with-identical-database-representations",
            "title": "dbt cannot create two resources with identical database representations",
            "body": "<p>I have a situation here as below:</p>\n<p>There are two models in my dbt project</p>\n<ol>\n<li>model-A</li>\n</ol>\n<pre><code>{{ config(\n    materialized='ephemeral',\n    alias='A_0001',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<ol start=\"2\">\n<li>model-B</li>\n</ol>\n<pre><code>{{ config(\n    materialized='ephemeral',\n    alias='B_0002',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<p>And these are getting materialized as incremental in same schema as <code>xxx_yyy_dataset.Table_DDD</code></p>\n<pre><code>{{ config(\n    materialized='incremental',\n    alias='Table_DDD',\n    schema=var('xxx_yyy_dataset')\n) }}\nSELECT * FROM {{ref('A_0001')}}\nUNION ALL\nSELECT * FROM {{ref('B_0002')}}\n</code></pre>\n<p>This is working fine and it is ingesting records into target table.</p>\n<p>Now I have introduced another model - model-C ind different package\nmodel-C</p>\n<pre><code>{{ config(\n    materialized='incremental',\n    alias='Table_DDD',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<p>This gives me the following error:</p>\n<pre><code>$ dbt compile --profiles-dir=profile --target ide\nRunning with dbt=0.16.0\nEncountered an error:\nCompilation Error\n  dbt found two resources with the database representation &quot;xxx_yyy_dataset.Table_DDD&quot;.\n  dbt cannot create two resources with identical database representations. To fix this,\n  change the &quot;schema&quot; or &quot;alias&quot; configuration of one of these resources:\n  - model.eplus_rnc_dbt_project.conrol_outcome_joined (models/controls/payment/fa-join/conrol_outcome_joined.sql)\n  - model.eplus_rnc_dbt_project.dq_control_outcome_joined (models/controls/dq/dq-join/dq_control_outcome_joined.sql)\n</code></pre>\n<p>I have configured macro for custom macro as below :</p>\n<pre><code>{% macro generate_schema_name(custom_schema_name, node) -%}\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n        {{ default_schema }}\n    {%- else -%}\n        {{ custom_schema_name }}\n    {%- endif -%}\n{%- endmacro %}\n\n\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n    {%- if custom_alias_name is none -%}\n        {{ node.name }}\n    {%- else -%}\n        {{ custom_alias_name | trim }}\n    enter code here\n    {%- endif -%}\n{%- endmacro %}\n</code></pre>\n"
        },
        {
            "tags": [
                "yaml",
                "dbt"
            ],
            "owner": {
                "account_id": 3873970,
                "reputation": 22234,
                "user_id": 4281353,
                "user_type": "registered",
                "accept_rate": 69,
                "profile_image": "https://www.gravatar.com/avatar/2c965e9fbb680e2527285a75b1712497?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "mon",
                "link": "https://stackoverflow.com/users/4281353/mon"
            },
            "is_answered": true,
            "view_count": 1682,
            "accepted_answer_id": 73876208,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1666822595,
            "creation_date": 1664338788,
            "last_edit_date": 1664358686,
            "question_id": 73876165,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73876165/dbt-what-is-in-yaml-files",
            "title": "dbt - what is +/- in YAML files?",
            "body": "<p>In DBT YAML file such as dbt_project.yml, what is <code>+</code> or <code>-</code> sign of the element?</p>\n<pre><code>models:\n  # Be sure to namespace your model configs to your project name\n  dbt_labs:\n\n    # This configures models found in models/events/\n    events:\n      +enabled: true            # &lt;--- What is the meaning of +?\n      +materialized: view       # &lt;--- What is the meaning of +?\n\n      # This configures models found in models/events/base\n      # These models will be ephemeral, as the config above is overridden\n      base:\n        +materialized: ephemeral       # &lt;--- What is the meaning of +?\n</code></pre>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "workflow",
                "dbt"
            ],
            "owner": {
                "account_id": 18054890,
                "reputation": 51,
                "user_id": 13123614,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/6510e38de5a659d76c3a318cc54f80fc?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Mirae Kim",
                "link": "https://stackoverflow.com/users/13123614/mirae-kim"
            },
            "is_answered": true,
            "view_count": 3133,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1653294409,
            "creation_date": 1643315064,
            "question_id": 70885262,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70885262/how-to-set-up-a-dev-to-prod-workflow-on-snowflake-and-dbt",
            "title": "How to set up a dev to prod workflow on snowflake (and dbt)?",
            "body": "<p>We are currently implementing snowflake and dbt and want to split snowflake databases between dev and prod, so that we have a database to test on before releasing new data models. We are planning to use dbt to create all of our data models going forward. I have a couple questions on the logistics of the workflow:</p>\n<ol>\n<li><p>How do we keep dev and prod in sync? (Or should they be?) I know in snowflake theres a clone feature you can recreate metadata without copying data over. Should we clone our prod database to dev? On a daily basis? What about users that have materialized resources in dev -- they would lose that data.</p>\n</li>\n<li><p>Should we make it so that deployment to prod was part of the CICD process, and only a fully merged pull request (tested on snowflake dev) can be deployed to the snowflake prod? Would that present too much of a bottle neck?</p>\n</li>\n</ol>\n<p>Curious to understand how people have architected their workflows maintaining both a dev and prod snowflake environment.</p>\n"
        },
        {
            "tags": [
                "yaml",
                "markdown",
                "github-flavored-markdown",
                "dbt",
                "mkdocs"
            ],
            "owner": {
                "account_id": 9319213,
                "reputation": 471,
                "user_id": 10713420,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/51001831aa59f592c10f668fcfb1c316?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "NAB0815",
                "link": "https://stackoverflow.com/users/10713420/nab0815"
            },
            "is_answered": true,
            "view_count": 2777,
            "accepted_answer_id": 69399550,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1633507139,
            "creation_date": 1632258585,
            "last_edit_date": 1633507139,
            "question_id": 69275594,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69275594/referencing-table-values-from-md-file-in-dbt-yaml-files",
            "title": "Referencing table values from md file in dbt yaml files",
            "body": "<p>I have a markdown file with a table in it and I have another dbt schema.yml file which is used to serve and generate docs. Traditionally I enter the table name and column name and description of the column name in schema.yml but now that I think of I'd want to reference the column names from the md document into the yaml file rather than manually entering.</p>\n<p>This is how my <code>doc.md</code> file looks like</p>\n<pre><code>{% docs column_description %}\n\n| COLUMN\\_NAME                   | DESCRIPTION                                                               |\n| ------------------------------ | ------------------------------------------------------------------------- |\n| cycle\\_id                      | Customer cycle\\_id(todays start sleep time to next days start sleep time) |\n| user\\_id                       | Customers user\\_id                                                        |\n| yes\\_alcohol                   | User consumed alcohol or not                                              |\n| blank\\_alcohol                 | User did not answer or user answered &quot;No&quot;                                 |\n                    \n\n\n{% enddocs %}\n</code></pre>\n<p>And, currently, this is how my schema.yml file looks</p>\n<pre><code>version: 2\n\nmodels:\n\n- name: journal_pivot_dev\n    description: One for for each journal entry of within one customer cycle\n    columns:\n      - name: cycle_id\n        description:  Customer cycleid\n        tests: &amp;not_null\n          - not_null:\n              severity: warn\n      - name: user_id\n        description: customer userid\n        tests:\n          - not_null:\n              severity: warn\n      - name: yes_alcohol\n        description: User consumed alcohol or not\n        tests: &amp;values_accepted\n          - accepted_values:\n              severity: warn\n              values: [0,1]\n      - name: blank_alcohol\n        description: User did not answer          \n        tests: *values_accepted \n</code></pre>\n<p>What I tried:</p>\n<pre><code>version: 2\n    \n    models:\n    \n    - name: journal_pivot_dev\n        description: One for for each journal entry of within one customer cycle\n        columns:\n          - name: cycle_id\n            description:  '{{ doc(column_description&quot;) }}'\n            tests: &amp;not_null\n              - not_null:\n                  severity: warn\n          - name: user_id\n            description: '{{ doc(column_description&quot;) }}'\n            tests:\n              - not_null:\n                  severity: warn\n          - name: yes_alcohol\n            description: '{{ doc(column_description&quot;) }}'\n            tests: &amp;values_accepted\n              - accepted_values:\n                  severity: warn\n                  values: [0,1]\n          - name: blank_alcohol\n            description: '{{ doc(column_description&quot;) }}'          \n            tests: *values_accepted \n</code></pre>\n<p>But when I do that the description is not rendering to just cycle_id but it is giving me the whole table in the md file.</p>\n<p><a href=\"https://i.sstatic.net/G8aQd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/G8aQd.png\" alt=\"enter image description here\" /></a></p>\n<p>I am expecting something like this</p>\n<p><a href=\"https://i.sstatic.net/Z7v7V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Z7v7V.png\" alt=\"enter image description here\" /></a></p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 16611383,
                "reputation": 51,
                "user_id": 12004301,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mCreIc1rUd8FdqL67UaVsfZXhLvlpqlTAwypGIS=k-s256",
                "display_name": "Ray Mar",
                "link": "https://stackoverflow.com/users/12004301/ray-mar"
            },
            "is_answered": true,
            "view_count": 3822,
            "answer_count": 3,
            "score": 5,
            "last_activity_date": 1679425315,
            "creation_date": 1609890872,
            "last_edit_date": 1610017183,
            "question_id": 65588257,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/65588257/macro-for-custom-schema-names-doesnt-apply-in-a-dbt-package",
            "title": "Macro for custom schema names doesn&#39;t apply in a dbt package",
            "body": "<p>I have an issue using custom schema names in a dbt package.</p>\n<p>I use the Macro provided in <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas/#how-does-dbt-generate-a-models-schema-name\" rel=\"noreferrer\">dbt documentation</a>.</p>\n<pre><code>{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n</code></pre>\n<p>I put this macro in my dbt package here <a href=\"https://github.com/datafuel/covid-france_dbt/tree/0.0.1\" rel=\"noreferrer\">dbt package</a>.</p>\n<p>Finally I use this dbt package in another dbt project <a href=\"https://github.com/datafuel/DataPlatform_docker/tree/feature/dbt-package/dbt_repo/src\" rel=\"noreferrer\">dbt project</a>.</p>\n<p>Here is my dbt_project.yml in my dbt project :</p>\n<pre><code>name: 'covid_france'\nversion: '0.0.1'\nconfig-version: 2\n\nprofile: 'default'\n\nsource-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analysis&quot;]\ntest-paths: [&quot;tests&quot;]\ndata-paths: [&quot;data&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  \nclean-targets:         \n    - &quot;target&quot;\n    - &quot;dbt_modules&quot;\n</code></pre>\n<p>My dbt_project.yml in my dbt package :</p>\n<pre><code>name: 'covid_france'\nversion: '0.0.1'\nconfig-version: 2\n\nprofile: 'default'\n\nsource-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analysis&quot;]\ntest-paths: [&quot;tests&quot;]\ndata-paths: [&quot;data&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  \nclean-targets:         \n    - &quot;target&quot;\n    - &quot;dbt_modules&quot;\n\nmodels:\n    covid_france:\n        stg:\n            materialized: table\n            schema: stg\n        ods:\n            materialized: table\n            process-airbyte-outputs:\n                schema: ods\n            unions:\n                schema: ods\n        prs:\n            materialized: view\n      \n</code></pre>\n<p>When I try to run my dbt project, it imports the dbt package but doesn't apply the macro that is supposed to remove the main schema prefix (provided in profiles.yml) from custom schema names\nFor instance : the schema provided in my profiles.yml is &quot;prs&quot;. I have other custom schemas named ods and stg. But when dbt run, it create prs, prs_ods and prs_stg.</p>\n<p>The macro used to work fine when I use it directly in a dbt project (instead of putting it in a dbt package that I use in my dbt project)</p>\n<p>Thank you in advance !</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 14567808,
                "reputation": 12018,
                "user_id": 10521959,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/DymiN.png?s=256",
                "display_name": "Yaakov Bressler",
                "link": "https://stackoverflow.com/users/10521959/yaakov-bressler"
            },
            "is_answered": true,
            "view_count": 1532,
            "accepted_answer_id": 77084777,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1718967091,
            "creation_date": 1694453323,
            "question_id": 77083700,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/77083700/disable-elementary-from-dbt-cli-when-running-dbt-test",
            "title": "Disable elementary from DBT CLI when running dbt test",
            "body": "<p>I have <a href=\"https://docs.elementary-data.com/understand-elementary/elementary-overview\" rel=\"noreferrer\">Elementary</a> installed in my DBT project:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code># packages.yml\npackages:\n  - package: elementary-data/elementary\n    version: 0.9.0\n\n# dbt_project.yml\nmodels:\n  elementary:\n    +schema: elementary\n</code></pre>\n<p>When I run DBT from the CLI, elementary will open connections and model executions, as well as test executions + results. <em><strong>I'd like to disable elementary's behavior, from the CLI.</strong></em> <em>(Not sure if it's possible...)</em></p>\n<p>An ideal solution would be a CLI flag:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>dbt run -s my_model --disable-elementary\n</code></pre>\n<p><em>(I do not want to alter my profiles... or dbt_project.yml)</em></p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 23108720,
                "reputation": 171,
                "user_id": 17220802,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/8be4ac303637cc833c46d2c32d443036?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Jb-99",
                "link": "https://stackoverflow.com/users/17220802/jb-99"
            },
            "is_answered": true,
            "view_count": 2839,
            "accepted_answer_id": 71889468,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1650061361,
            "creation_date": 1649727504,
            "last_edit_date": 1649729076,
            "question_id": 71836051,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71836051/declaring-multiple-warehouses-in-dbt",
            "title": "Declaring multiple warehouses in dbt",
            "body": "<p>I am pretty new to dbt , i want to use two kinds of warehouses in one project , currently i declared my clickhouse warehouse  which i am going to make tables for and i need to add another warehouse mindsDB becuase i want to reference some of the tables in it</p>\n<p>currently my prfofile.yml looks like this</p>\n<pre><code>dbt-project:\n  target: dev\n  outputs:\n    dev:\n      type: clickhouse\n      schema : clickhouse_l\n      host: 8.77.780.70\n      port: 6000\n      user: xxx\n      password: xxxx\n</code></pre>\n<p>i want to add the below warehouse too</p>\n<pre><code>type: mysql\nhost: mysql.mindsdb.com\nuser: mindsdb.user@example.com\npassword: xxx\nport: 3306\ndbname: mindsdb\nschema: exampl_xxx\nthreads: 1\n</code></pre>\n<p>is there a way to do it? thank you</p>\n"
        },
        {
            "tags": [
                "jinja2",
                "snowflake-cloud-data-platform",
                "dbt"
            ],
            "owner": {
                "account_id": 3690699,
                "reputation": 2763,
                "user_id": 3073340,
                "user_type": "registered",
                "accept_rate": 95,
                "profile_image": "https://www.gravatar.com/avatar/114cd763dfcabbe73f1aa28f95a7eb1b?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "sgdata",
                "link": "https://stackoverflow.com/users/3073340/sgdata"
            },
            "is_answered": true,
            "view_count": 1738,
            "accepted_answer_id": 70371015,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1639607068,
            "creation_date": 1633035594,
            "last_edit_date": 1633400909,
            "question_id": 69398772,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69398772/is-there-a-variable-list-of-all-valid-databases-schemas-combinations-in-dbt",
            "title": "Is there a variable list of all *valid* databases &amp; schemas combinations in dbt jinja?",
            "body": "<p>Doing a variation on <a href=\"https://discourse.getdbt.com/t/setting-up-snowflake-the-exact-grant-statements-we-run/439\" rel=\"nofollow noreferrer\">this example</a> for a macro (<code>grant_select_on_schemas.sql</code>) to set grants on a snowflake instance after dbt runs. My issue is that I've inherited a non-standard dbt build configuration which includes some statically defined <em>non-target</em> model locations.</p>\n<p>Examples:</p>\n<pre><code>snowflake-instance\n    |\n    |&gt; raw_db\n        |&gt; elt_schema_1\n        |&gt; elt_schema_2\n        |&gt; elt_schema_3\n    |&gt; utils_db\n        |&gt; calendar_schema_1\n    |&gt; staging_db\n        |&gt; elt_staging_1\n        |&gt; elt_staging_2\n        |&gt; elt_staging_3\n    |&gt; analytics_db\n        |&gt; core_models\n        |&gt; mart_1\n        |&gt; mart_2\n</code></pre>\n<p><code>profiles.yml</code></p>\n<pre><code>  target: prod\n  outputs:\n    prod:\n      type: snowflake\n      account: my-account.region-1\n      role: my-role\n\n      # User/password auth\n      user: &lt;user&gt;\n      password: &lt;pass&gt;\n\n      database: raw_db\n      warehouse: my-warehouse\n      schema: PUBLIC\n      threads: 2\n      client_session_keep_alive: False\n      query_tag: my-dbt-local\n</code></pre>\n<p><code>dbt-project.yml</code></p>\n<pre><code>models:\n    my-pro:\n        +materialized: table   \n        utils:\n            +database: UTILS\n            +materialized: table\n            calendar:\n                +schema: calendar_schema_1\n        staging:\n            +database: staging_db\n            +materialized: view\n            elt_staging_1:\n                +schema: elt_staging_1\n            elt_staging_2:\n                +schema: elt_staging_2\n            elt_staging_3:\n                +schema: elt_staging_3\n\n</code></pre>\n<p><code>grant_select_on_schemas.sql</code></p>\n<pre><code>-- macros/grants/grant_select_on_schemas.sql\n\n{% macro grant_select_on_schemas(schemas, role) %}\n  {% for schema in schemas %}\n    {% for role in roles %}\n      grant usage on schema {{ schema }} to role {{ role }};\n      grant select on all tables in schema {{ schema }} to role {{ role }};\n      grant select on all views in schema {{ schema }} to role {{ role }};\n      grant select on future tables in schema {{ schema }} to role {{ role }};\n      grant select on future views in schema {{ schema }} to role {{ role }};\n    {% endfor %}\n  {% endfor %}\n{% endmacro %}\n</code></pre>\n<p>Currently, I'm running into the issue with this macro that the macro is attempting to run on against all schemas on my profile's <code>{{ target.database }}</code> (which is currently set to <code>staging_db</code>) and as a consequence is erroring when attempting things like:</p>\n<pre><code>&gt; Database Error\n&gt;   002003 (02000): SQL compilation error:\n&gt;   Schema 'staging_db.core_models' does not exist or not authorized.\n</code></pre>\n<p>What am I missing?</p>\n"
        },
        {
            "tags": [
                "stored-procedures",
                "dbt",
                "snowflake-cloud-data-platform"
            ],
            "owner": {
                "account_id": 4920634,
                "reputation": 63,
                "user_id": 3963284,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/19585b71b4d21f462c717b224fa83209?s=256&d=identicon&r=PG",
                "display_name": "Bardia",
                "link": "https://stackoverflow.com/users/3963284/bardia"
            },
            "is_answered": true,
            "view_count": 3656,
            "accepted_answer_id": 69444774,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1636637668,
            "creation_date": 1631611894,
            "question_id": 69175318,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69175318/snowflake-stored-procedure-fails-from-dbt",
            "title": "Snowflake stored procedure fails from dbt",
            "body": "<p>I have a problem to execute a stored procedure in Snowflake by dbt:</p>\n<p>The description of my procedure is like: MyStoredProcedure(ARRAY, VARCHAR, VARCHAR)</p>\n<p>So, when I want to run it, I use <strong>array_construct</strong> function to create the first argument, for example:\n<code>call MyStoredProcedure(array_construct(array_construct('str_1', 'str_2')), 'schema_name', 'table_name');</code></p>\n<p>This works when I run it in Snowflake. However, when I run this from dbt it fails with this error:</p>\n<blockquote>\n<p>Modifying a transaction that has started at a different scope is not allowed.</p>\n</blockquote>\n<p>Which I am sure that it is something related to invoking array_construct in this call.</p>\n<p>I should mention that to run this from dbt I have defined a macro like this:</p>\n<pre><code>{% macro MyStoredProcedure() %}\n    {% set query -%}\n        CALL MyStoredProcedure(\n           array_construct(array_construct('str_1', 'str_2')),\n           'schema_name',\n           'table_name');\n    {%- endset %}\n\n    {% do run_query(query) %}\n{% endmacro %}\n</code></pre>\n<p>And run it of course like: <code>dbt run-operation MyStoredProcedure</code></p>\n<p>I appreciate any tip or idea to help me with this problem.</p>\n<p>Thanks</p>\n"
        },
        {
            "tags": [
                "google-bigquery",
                "dbt"
            ],
            "owner": {
                "account_id": 13223066,
                "reputation": 123,
                "user_id": 9549147,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/u2q4t.jpg?s=256",
                "display_name": "Soner Guzeloglu",
                "link": "https://stackoverflow.com/users/9549147/soner-guzeloglu"
            },
            "is_answered": true,
            "view_count": 2012,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1704725435,
            "creation_date": 1630308460,
            "question_id": 68980333,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/68980333/duplicate-records-on-first-run-with-dbts-incremental-model-on-bigquery",
            "title": "Duplicate Records on First run with dbt&#39;s Incremental Model on BigQuery",
            "body": "<p>On one of the use case we have on our organisation, we have <code>incremental</code> tables which basically hold the append-only records of incoming events and <code>current</code> tables which stores the up-to-date state of incremental records with a unique key.</p>\n<p>Above use-case looked us like an exact match to implement incremental model for good.</p>\n<p>Documentation <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models\" rel=\"noreferrer\">here</a> states that;</p>\n<pre><code>...the first time a model is run, the table is built by transforming all rows of source data.\n</code></pre>\n<p>Because we have more than one record with the same unique key on our append-only incremental tables, the first incremental run generates more than one record with same unique key on current tables. Hence, consecutive batch following error;</p>\n<pre><code>UPDATE/MERGE must match at most one source row for each target row\n</code></pre>\n<p>Could anyone please let me know how this issue can be addressed with a solution or am I missing something?</p>\n<p>Thanks in advance,\nSoner</p>\n"
        },
        {
            "tags": [
                "databricks",
                "dbt"
            ],
            "owner": {
                "account_id": 6729535,
                "reputation": 1934,
                "user_id": 5682512,
                "user_type": "registered",
                "accept_rate": 73,
                "profile_image": "https://www.gravatar.com/avatar/0b543a3897886a2bee848c0a72598144?s=256&d=identicon&r=PG",
                "display_name": "k88",
                "link": "https://stackoverflow.com/users/5682512/k88"
            },
            "is_answered": true,
            "view_count": 3507,
            "accepted_answer_id": 71206919,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1648047594,
            "creation_date": 1644247109,
            "last_edit_date": 1645173887,
            "question_id": 71020949,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71020949/cant-connect-dbt-to-databricks",
            "title": "Can&#39;t connect dbt to Databricks",
            "body": "<p>I am trying to connect to a Spark cluster on Databricks and I am following this tutorial: <a href=\"https://docs.databricks.com/dev-tools/dbt.html\" rel=\"nofollow noreferrer\">https://docs.databricks.com/dev-tools/dbt.html</a>. And I have the <code>dbt-databricks</code> connector installed (<a href=\"https://github.com/databricks/dbt-databricks\" rel=\"nofollow noreferrer\">https://github.com/databricks/dbt-databricks</a>). However, no matter how I configure it, I keep getting &quot;Database error, failed to connect&quot; when I run <code>dbt test</code> / <code>dbt debug</code>.</p>\n<p>This is my <code>profiles.yaml</code>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>databricks_cluster:\n  outputs:\n    dev:\n      connect_retries: 5\n      connect_timeout: 60\n      host: &lt;my_server_hostname&gt;\n      http_path: &lt;my_http_path&gt;\n      schema: default\n      token: &lt;my_token&gt;\n      type: databricks\n  target: dev\n</code></pre>\n<p>This is my <code>dbt_project.yml</code>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'dbt_dem'\nversion: '1.0.0'\nconfig-version: 2\n\n# This setting configures which &quot;profile&quot; dbt uses for this project.\nprofile: 'databricks_cluster'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the &quot;models/&quot; directory. You probably won't need to change these!\nmodel-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analyses&quot;]\ntest-paths: [&quot;tests&quot;]\nseed-paths: [&quot;seeds&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  # directory which will store compiled SQL files\nclean-targets:         # directories to be removed by `dbt clean`\n  - &quot;target&quot;\n  - &quot;dbt_packages&quot;\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/ directory\n# as tables. These settings can be overridden in the individual model files\n# using the `{{ config(...) }}` macro.\nmodels:\n  dbt_dem:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre>\n<p>I have also tried using the <code>spark</code> connector, but I still get the same error using that. Any ideas as to why I can't connect to the Databricks cluster?</p>\n<p>These are the logs corresponding to the error:</p>\n<pre><code>============================== 2022-02-18 08:43:22.123066 | 4b91f9d3-28ad-4f5a-93db-f431b6d9af14 ==============================\n08:43:22.123066 [info ] [MainThread]: Running with dbt=1.0.1\n08:43:22.123841 [debug] [MainThread]: running dbt with arguments Namespace(cls=&lt;class 'dbt.task.debug.DebugTask'&gt;, config_dir=False, debug=None, defer=None, event_buffer_size=None, fail_fast=None, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='/Users/keremaslan/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='debug', write_json=None)\n08:43:22.124057 [debug] [MainThread]: Tracking: tracking\n08:43:22.143750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751ef42e0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751ef4eb0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751eca730&gt;]}\n08:43:22.236001 [debug] [MainThread]: Executing &quot;git --help&quot;\n08:43:22.264682 [debug] [MainThread]: STDOUT: &quot;b&quot;usage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\\n           &lt;command&gt; [&lt;args&gt;]\\n\\nThese are common Git commands used in various situations:\\n\\nstart a working area (see also: git help tutorial)\\n   clone             Clone a repository into a new directory\\n   init              Create an empty Git repository or reinitialize an existing one\\n\\nwork on the current change (see also: git help everyday)\\n   add               Add file contents to the index\\n   mv                Move or rename a file, a directory, or a symlink\\n   restore           Restore working tree files\\n   rm                Remove files from the working tree and from the index\\n   sparse-checkout   Initialize and modify the sparse-checkout\\n\\nexamine the history and state (see also: git help revisions)\\n   bisect            Use binary search to find the commit that introduced a bug\\n   diff              Show changes between commits, commit and working tree, etc\\n   grep              Print lines matching a pattern\\n   log               Show commit logs\\n   show              Show various types of objects\\n   status            Show the working tree status\\n\\ngrow, mark and tweak your common history\\n   branch            List, create, or delete branches\\n   commit            Record changes to the repository\\n   merge             Join two or more development histories together\\n   rebase            Reapply commits on top of another base tip\\n   reset             Reset current HEAD to the specified state\\n   switch            Switch branches\\n   tag               Create, list, delete or verify a tag object signed with GPG\\n\\ncollaborate (see also: git help workflows)\\n   fetch             Download objects and refs from another repository\\n   pull              Fetch from and integrate with another repository or a local branch\\n   push              Update remote refs along with associated objects\\n\\n'git help -a' and 'git help -g' list available subcommands and some\\nconcept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'\\nto read about a specific subcommand or concept.\\nSee 'git help git' for an overview of the system.\\n&quot;&quot;\n08:43:22.265387 [debug] [MainThread]: STDERR: &quot;b''&quot;\n08:43:22.272505 [debug] [MainThread]: Acquiring new databricks connection &quot;debug&quot;\n08:43:22.273434 [debug] [MainThread]: Using databricks connection &quot;debug&quot;\n08:43:22.273833 [debug] [MainThread]: On debug: select 1 as id\n08:43:22.274044 [debug] [MainThread]: Opening a new connection, currently in state init\n08:43:22.888586 [debug] [MainThread]: Databricks adapter: Error while running:\nselect 1 as id\n08:43:22.889031 [debug] [MainThread]: Databricks adapter: Database Error\n  failed to connect\n08:43:22.889905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751f7eaf0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb752113040&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7521130a0&gt;]}\n08:43:24.130154 [debug] [MainThread]: Connection 'debug' was properly closed.\n</code></pre>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "yaml",
                "dbt"
            ],
            "owner": {
                "account_id": 24346813,
                "reputation": 273,
                "user_id": 18283242,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/wZcQP.jpg?s=256",
                "display_name": "Aliaksandra",
                "link": "https://stackoverflow.com/users/18283242/aliaksandra"
            },
            "is_answered": false,
            "view_count": 1704,
            "answer_count": 0,
            "score": 5,
            "last_activity_date": 1728906454,
            "creation_date": 1654796084,
            "question_id": 72564447,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72564447/how-to-add-description-to-the-test-in-dbt",
            "title": "how to add description to the test in dbt",
            "body": "<p>How to add description to the generic test which is usually written in yml so that I can see it in the DESCRIPTION field in dbt docs</p>\n<p>in yml such tests are usually written like this but how to add a description to the test</p>\n<pre><code>        - name: desc\n         description: &quot;it is the description of the column but not the test&quot;\n          tests:\n          - not_null:\n</code></pre>\n<p><a href=\"https://i.sstatic.net/xpYko.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xpYko.png\" alt=\"enter image description here\" /></a></p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 6802882,
                "reputation": 595,
                "user_id": 5235665,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/b1b829c8707d74031ecd37125d6d2f16?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "hotmeatballsoup",
                "link": "https://stackoverflow.com/users/5235665/hotmeatballsoup"
            },
            "is_answered": true,
            "view_count": 695,
            "accepted_answer_id": 76519301,
            "answer_count": 2,
            "score": 5,
            "last_activity_date": 1696591488,
            "creation_date": 1686942733,
            "last_edit_date": 1686943068,
            "question_id": 76492997,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/76492997/automatically-generated-python-dbt-test-seems-to-be-hardcoded-to-fail",
            "title": "Automatically-generated Python dbt test seems to be hardcoded to fail",
            "body": "<p>I have a Python DBT project that defines the following data model (via YAML):</p>\n<pre><code>version: 2\nmodels:\n- name: company\n  description: ''\n  columns:\n  - name: ID\n    description: Unique identifier for the company entity. It is the company name\n    tests:\n      - unique\n      - not_null\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: VARCHAR\n  - name: REMOTE_ID\n    description: Company name\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: VARCHAR\n  - name: CREATED_AT\n    description: ''\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: TIMESTAMP_TZ\n      - not_null\n  - name: UPDATED_AT\n    description: ''\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: TIMESTAMP_TZ\n</code></pre>\n<p>When I run <code>dbt compile</code>, then <code>dbt run</code> then <code>dbt test</code> I get a DBT test failure with the following console output:</p>\n<pre><code>17:59:30  Failure in test dbt_expectations_expect_column_values_to_be_of_type_company_CREATED_AT__TIMESTAMP_TZ (models/fizzbuzz/domain/company.yml)\n17:59:30    Got 1 result, configured to fail if != 0\n17:59:30  \n17:59:30    compiled Code at target/compiled/myapp/models/fizzbuzz/domain/company.yml/dbt_expectations_expect_column_837854ce21e79ee1abe42f69891c68e9.sql\n</code></pre>\n<p>When I go to that compiled SQL at <code>dbt_expectations_expect_column_837854ce21e79ee1abe42f69891c68e9.sql</code> this is what I see:</p>\n<pre><code>WITH relation_columns\n     AS (SELECT Cast('ID' AS VARCHAR)      AS relation_column,\n                Cast('VARCHAR' AS VARCHAR) AS relation_column_type\n         UNION ALL\n         SELECT Cast('REMOTE_ID' AS VARCHAR) AS relation_column,\n                Cast('VARCHAR' AS VARCHAR)   AS relation_column_type\n         UNION ALL\n         SELECT Cast('CREATED_AT' AS VARCHAR)    AS relation_column,\n                Cast('TIMESTAMP_NTZ' AS VARCHAR) AS relation_column_type\n         UNION ALL\n         SELECT Cast('UPDATED_AT' AS VARCHAR)    AS relation_column,\n                Cast('TIMESTAMP_NTZ' AS VARCHAR) AS relation_column_type),\n     test_data\n     AS (SELECT *\n         FROM   relation_columns\n         WHERE  relation_column = 'CREATED_AT'\n                AND relation_column_type NOT IN ( 'TIMESTAMP_TZ' ))\nSELECT *\nFROM   test_data \n</code></pre>\n<p>So my <em>understanding</em> is that when I run <code>dbt compile</code> it generates this test above from the compiled YAML file definition. And that when I subsequently run <code>dbt test</code>, it runs this SQL to determine whether the test defined in that YAML passes or fails.</p>\n<p>If that's correct, don't we have a bug in DBT here? Is it me or isn't this a false negative test that is hard-coded to always fail?!</p>\n<p>It's creating a temp table/CTE called <code>test_data</code> that is in no way, shape or form connecting to my <code>company</code> database table, but that has 2 columns (<code>relation_column</code> and <code>relation_column_type</code>). Then its giving this table a row where the <code>relation_column</code> value is &quot;CREATED_AT&quot; and its <code>relation_column_type</code> value is &quot;TIMESTAMP_NTZ&quot;. And finally its returning any records whose <code>relation_column</code> value is &quot;CREATED_AT&quot; but whose <code>relation_column_type</code> value is <strong>not</strong> &quot;TIMESTAMP_NTZ&quot;...so wouldn't this always fail?!</p>\n<p>Or am I missing something <strong>big</strong> about how DBT and its tests work? Thanks for any clarifications here.</p>\n"
        },
        {
            "tags": [
                "api",
                "data-analysis",
                "transformation",
                "data-transform",
                "dbt"
            ],
            "owner": {
                "account_id": 5036339,
                "reputation": 6385,
                "user_id": 4044988,
                "user_type": "registered",
                "accept_rate": 40,
                "profile_image": "https://lh3.googleusercontent.com/-HexM6y2dAAU/AAAAAAAAAAI/AAAAAAAAABI/0iGjyusWH8A/photo.jpg?sz=256",
                "display_name": "MegaBytes",
                "link": "https://stackoverflow.com/users/4044988/megabytes"
            },
            "is_answered": true,
            "view_count": 8765,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1651033364,
            "creation_date": 1613479209,
            "question_id": 66224513,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/66224513/can-we-call-any-external-rest-api-inside-dbtdata-build-tool",
            "title": "Can we call any external REST API inside DBT(Data Build Tool)?",
            "body": "<p>I am working on some analytical work and we need to transform data from one source to another and we are using <a href=\"https://www.getdbt.com/\" rel=\"nofollow noreferrer\">DBT</a> for transformation purpose. one of the data available to use via only REST API. so my question is can we call external API inside dbt file and extract the fields from its response. Do we have something?</p>\n"
        },
        {
            "tags": [
                "sql",
                "jinja2",
                "dbt",
                "jinja2-cli"
            ],
            "owner": {
                "account_id": 19149486,
                "reputation": 51,
                "user_id": 13987714,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ed716031526b41ddf3ed8f439f8c6b89?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Angel Dimov",
                "link": "https://stackoverflow.com/users/13987714/angel-dimov"
            },
            "is_answered": true,
            "view_count": 8013,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1595612264,
            "creation_date": 1595582928,
            "last_edit_date": 1595596792,
            "question_id": 63070763,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/63070763/create-multiple-tables-in-bigquery-using-dbt-for-loop",
            "title": "Create Multiple Tables in BigQuery Using dbt for-loop",
            "body": "<p>I am trying to create individual tables inside a single dataset in BigQuery using a for-loop in dbt, going through a list of accounts, with no success so far.\nA little bit of context - I am using Stitch to fetch data from Facebook Ads and to push it to our BigQuery warehouse. Then, based on the model below, create new separate table for each account with aggregated/modelled data.</p>\n<p>The declaration of the variables looks like:</p>\n<pre><code>-- table that contains list of accounts\n{% set account_data = ref('bq_acct_list') %} \n{% set accounts = get_column_values(table=account_data, column='bq_name_suffix') %}\n</code></pre>\n<p>And the query that the tables have to created based on is:</p>\n<pre><code>SELECT \n        DATE_TRUNC(DATE(date_start), DAY) date,\n        account_id,\n        account_name,\n        ROUND(SUM(spend), 2) ad_spend\nFROM `{{ target.project }}.{{account}}.ads_insights`\nGROUP BY 1, 2, 3\n</code></pre>\n<p>What is missing (I think) is the wrapper of the query + the for-loop itself. Can anyone help me fill in the blanks?</p>\n"
        },
        {
            "tags": [
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 21860059,
                "reputation": 43,
                "user_id": 16150972,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/9646241412eec3c002902f4fc364b349?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "joboro",
                "link": "https://stackoverflow.com/users/16150972/joboro"
            },
            "is_answered": true,
            "view_count": 7740,
            "accepted_answer_id": 70789055,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1697640905,
            "creation_date": 1636988248,
            "last_edit_date": 1636989951,
            "question_id": 69976423,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69976423/dbt-utils-surrogate-key-with-all-fields-except-two",
            "title": "dbt_utils.surrogate_key() with all fields except two",
            "body": "<p>I am currently playing with dbt and trying to build a PSA (Persistent Staging Area) and the snapshot functionality lends itself well to this in my eyes. However, I don't have timestamps in the sources, so I have to use the &quot;check&quot; strategy.</p>\n<p>For &quot;check_cols&quot; I would like to use a hash value, so I thought of dbt_utils.surrogate_key().</p>\n<p>But I would like to calculate the hash value over all columns except the two columns that are always the same.</p>\n<p>So my model looks like this:</p>\n<pre><code>{% snapshot Item_hist %}\n{{\n    config(\n      unique_key='item_id',\n      strategy='check',\n      check_cols=['diff_hash'],\n      target_database='PSA',\n      target_schema='sourceA',\n      alias= 'Item',\n      invalidate_hard_deletes=True\n    )\n}}\n\nselect {{ dbt_utils.surrogate_key(['Tenant','ItemNo']) }} as item_id,\n{{ dbt_utils.surrogate_key( dbt_utils.star(from=source('sourceA', 'item'), except=[&quot;fieldA&quot;, &quot;fieldB&quot;]) ) }} as diff_hash,\n* \nfrom {{ source('sourceA', 'item') }}\n{% endsnapshot %}\n</code></pre>\n<p>Unfortunately, the dbt_utils.surrogate_key() cannot handle the return value of dbt_utils.star().\nHow could I proceed here so that surrogate_key() can calculate a hash value from the return?</p>\n"
        },
        {
            "tags": [
                "docker",
                "dbt"
            ],
            "owner": {
                "account_id": 15622610,
                "reputation": 2306,
                "user_id": 11271048,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/8thDR.jpg?s=256",
                "display_name": "alt-f4",
                "link": "https://stackoverflow.com/users/11271048/alt-f4"
            },
            "is_answered": true,
            "view_count": 11510,
            "accepted_answer_id": 65489276,
            "answer_count": 6,
            "score": 4,
            "last_activity_date": 1695911189,
            "creation_date": 1609088641,
            "last_edit_date": 1609235400,
            "question_id": 65468231,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/65468231/how-can-i-configure-dbt-dependencies-without-manually-running-dbt-deps",
            "title": "How can I configure DBT Dependencies without manually running dbt deps?",
            "body": "<p>I am new to DBT and currently trying to build a Docker container where I can directly run DBT commands within. I have a file where I export env variables (<code>envs.sh</code>) that looks like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>export DB_HOST=&quot;secret&quot;\nexport DB_PWD=&quot;evenabiggersecret&quot;\n</code></pre>\n<p>My <code>packages.yml</code> looks like:</p>\n<pre><code>packages:\n  - package: fishtown-analytics/dbt_utils\n    version: 0.6.2\n</code></pre>\n<p>I structured my docker file like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM fishtownanalytics/dbt:0.19.0b1\n# Define working directory\nWORKDIR /usr/app/profile/\nENV DBT_DIR /usr/app\nENV DBT_PROFILES_DIR /usr/app\n# Load ENV Vars\nCOPY ./dbt ${DBT_DIR}\n# Load env variables and install packages\nCOPY envs.sh envs.sh\nRUN . ./envs.sh \\\n &amp;&amp; dbt deps # Exporting envs to avoid profile not found errors when install deps\n</code></pre>\n<p>However, when I run <code>dbt run</code> inside the docker container I get the error:\n<code>'dbt_utils' is undefined</code>. When I manually run <code>dbt deps</code> it seems to fix the issue and <code>dbt run</code> succeeds. Am I missing something when I am originally installing the dependencies?</p>\n<p>Update:\nIn other words, running <code>dbt deps</code> when building the Docker image seems to have no effect. So I have to run it manually (when I do docker run for example) before I can start doing my workflows. This issue does not happen when I use a Python image (not the image from fishtown-analytics)</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 25367234,
                "reputation": 41,
                "user_id": 19181640,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi6gjjbC-HNcD92lel5ggeJ8DqYJdKWvncwIser=k-s256",
                "display_name": "Jason McKenzie",
                "link": "https://stackoverflow.com/users/19181640/jason-mckenzie"
            },
            "is_answered": true,
            "view_count": 4086,
            "answer_count": 3,
            "score": 4,
            "last_activity_date": 1721790992,
            "creation_date": 1653322526,
            "question_id": 72351740,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72351740/dbt-deps-the-process-cannot-access-the-file-because-it-is-being-used-by-anothe",
            "title": "dbt deps - The process cannot access the file because it is being used by another process: &#39;dbt_packages\\\\dbt-expectations-0.5.1\\\\integration_tests&#39;",
            "body": "<p>I last used dbt several months ago.  Returning to it, I had to update from version 0.21.1 to 1.1.0 to match my team's project.  I've run dbt clean, then dbt deps always returns this error: <a href=\"https://i.sstatic.net/WIWBE.png\" rel=\"nofollow noreferrer\">The process cannot access the file because it is being used by another process: 'dbt_packages\\dbt-expectations-0.5.1\\integration_tests'</a></p>\n<p>I see two packages after it errors:  dbt_expectations &amp; dbt-expectations-0.5.1 - I'm not sure what the latter package is about since it doesn't exist on my teammates machines.</p>\n<p>These are all my packages which match my teammates:</p>\n<ul>\n<li>package: calogica/dbt_date\nversion: 0.5.1</li>\n<li>package: dbt-labs/dbt_utils\nversion: 0.8.0</li>\n<li>package: dbt-labs/codegen\nversion: 0.5.0</li>\n<li>package: calogica/dbt_expectations\nversion: 0.5.1</li>\n</ul>\n<p>There's nothing else using the mentioned file before executing dbt deps &amp; I've restarted to ensure there wasn't anything hung up.  I don't see anything helpful in the logs:</p>\n<blockquote>\n<p>14:28:05.062468 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'package', 'label':\n'5cc0ad0f-792a-49a6-8a66-f59ff8c3e642', 'property_': 'install',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x000002CDFB6D1EB0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB6D14C0&gt;]} 14:28:05.063465 [info ] [MainThread]: Installing\ncalogica/dbt_expectations 14:28:06.961306 [debug] [MainThread]:\nSending event: {'category': 'dbt', 'action': 'invocation', 'label':\n'end', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB689550&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB6D14C0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB702EE0&gt;]}</p>\n<p>============================== 2022-05-23 14:28:10.424049 | 2a7e240e-1707-4551-8966-58797e038d3a ==============================\n14:28:10.424049 [info ] [MainThread]: Running with dbt=1.1.0\n14:28:10.425046 [debug] [MainThread]: running dbt with arguments\n{'write_json': True, 'use_colors': True, 'printer_width': 80,\n'version_check': True, 'partial_parse': True, 'static_parser': True,\n'profiles_dir': 'C:\\Users\\jasonmckenzie\\.dbt',\n'send_anonymous_usage_stats': True, 'event_buffer_size': 100000,\n'quiet': False, 'no_print': False, 'resource_types': [], 'output':\n'selector', 'indirect_selection': 'eager', 'which': 'list'}\n14:28:10.425046 [debug] [MainThread]: Tracking: tracking\n14:28:10.435018 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'invocation', 'label': 'start', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F3D90&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F32E0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F37C0&gt;]} 14:28:10.462945 [debug] [MainThread]: Sending\nevent: {'category': 'dbt', 'action': 'invocation', 'label': 'end',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x00000254727D1820&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727D1490&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727D19D0&gt;]}</p>\n<p>============================== 2022-05-23 14:28:10.825973 | 8d0ab879-6b59-4473-845a-cde9dd23352b ==============================\n14:28:10.825973 [info ] [MainThread]: Running with dbt=1.1.0\n14:28:10.826970 [debug] [MainThread]: running dbt with arguments\n{'write_json': True, 'use_colors': True, 'printer_width': 80,\n'version_check': True, 'partial_parse': True, 'static_parser': True,\n'profiles_dir': 'C:\\Users\\jasonmckenzie\\.dbt',\n'send_anonymous_usage_stats': True, 'event_buffer_size': 100000,\n'quiet': False, 'no_print': False, 'resource_types': [], 'output':\n'selector', 'indirect_selection': 'eager', 'which': 'list'}\n14:28:10.826970 [debug] [MainThread]: Tracking: tracking\n14:28:10.835946 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'invocation', 'label': 'start', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052F100&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052FEB0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052FC40&gt;]} 14:28:10.892797 [debug] [MainThread]: Sending\nevent: {'category': 'dbt', 'action': 'invocation', 'label': 'end',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x00000170D04EC640&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D04ECD90&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D04EC4F0&gt;]}</p>\n</blockquote>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 402310,
                "reputation": 1039,
                "user_id": 769405,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://www.gravatar.com/avatar/5f2d2d9093f8faac47b2321afe5cfc5d?s=256&d=identicon&r=PG",
                "display_name": "mikelus",
                "link": "https://stackoverflow.com/users/769405/mikelus"
            },
            "is_answered": true,
            "view_count": 4338,
            "accepted_answer_id": 72047969,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1683560775,
            "creation_date": 1651146879,
            "question_id": 72043221,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72043221/why-is-dbt-running-a-model-that-is-not-being-targeted-explicitly-in-the-dbt-run",
            "title": "Why is DBT running a model that is not being targeted explicitly in the DBT run statement?",
            "body": "<p>I have a DBT project that is mostly comprised of models for views over snowflake external tables.Every model view is triggered with a seperate dbt run statement concurrently.</p>\n<pre><code>dbt run --models model_for_view_1\n</code></pre>\n<p>I have one other model in the dbt project which materializes to a table that uses these views. I trigger this model in a separate DAG in airflow using the same DBT run statement as above. It uses no ref or source statement that connects it to the views.</p>\n<p>I noticed recently that this table model is getting built by DBT whenever I build the view models. I thought it was because DBT was making an inference that this was a referenced model but after some experimentation in which I even set the table model SQL as something like SELECT 1+1 as column1, it was still getting built. I have placed it in a different folder in the dbt project, renamed the file etc. No joy. have no idea why running the other models is causing this unrelated model to be built. The only connection to the view models is that they share the same schema in the database. What is triggering this model to be built?</p>\n"
        },
        {
            "tags": [
                "variables",
                "yaml",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 15705200,
                "reputation": 115,
                "user_id": 11332127,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-w4LN0XNtDhk/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rd6aR90Kl4bTWjOLooRhd4hXJat7Q/mo/photo.jpg?sz=256",
                "display_name": "Mrinal",
                "link": "https://stackoverflow.com/users/11332127/mrinal"
            },
            "is_answered": true,
            "view_count": 3460,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1649194665,
            "creation_date": 1638272611,
            "last_edit_date": 1638290193,
            "question_id": 70168808,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70168808/nested-variables-in-dbt-project-yml-file-of-dbt",
            "title": "Nested variables in dbt_project.yml file of dbt",
            "body": "<p>Below is my code in dbt_project.yml file</p>\n<pre><code>vars:\n    # Variable \n    project1:\nvendor:\n    ABC\n    DEF\n\nABC:\n    model:\n        name: model123\n    case_types:\n        name: CASE1\n        name: CASE2\n        name: CASE3\n        name: CASE4\n</code></pre>\n<p>the way i am trying to access this in model is below. When the below code is run the values in src are ('model' &amp; 'case_types') respectively. How do i access values of these( 'model123, CASE1, CASE2....)</p>\n<pre><code>{% set vars1 =  var('ABC') %}\n{% for src in vars1 %}\n  {{log(src, True)}}\n{% endfor %}\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 21862411,
                "reputation": 51,
                "user_id": 16153059,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AATXAJwb8vicWjBSWrCmeLlTBTIHe11NNujNYgasRaDD=k-s256",
                "display_name": "rodda mamatha",
                "link": "https://stackoverflow.com/users/16153059/rodda-mamatha"
            },
            "is_answered": true,
            "view_count": 19512,
            "answer_count": 4,
            "score": 4,
            "last_activity_date": 1723815568,
            "creation_date": 1623062804,
            "question_id": 67870208,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/67870208/how-to-use-update-statement-in-dbt",
            "title": "How to use Update statement in DBT",
            "body": "<p>How to use Update statment in DBT?\nWe created table in Snowflake using DBT but unable to use Update query to update the same table.</p>\n<p>Is there any other way that we can achieve this , like other ELT/ETL tools?</p>\n"
        },
        {
            "tags": [
                "python",
                "etl",
                "dbt"
            ],
            "owner": {
                "account_id": 17979288,
                "reputation": 351,
                "user_id": 13066054,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gh6MZsCRyDro27I5q923fET00GK6T2-b6e-VpUswA=k-s256",
                "display_name": "naga satish",
                "link": "https://stackoverflow.com/users/13066054/naga-satish"
            },
            "is_answered": true,
            "view_count": 11017,
            "accepted_answer_id": 74855292,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1671618670,
            "creation_date": 1671450645,
            "last_edit_date": 1671618670,
            "question_id": 74850128,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74850128/macros-are-not-recognised-in-dbt",
            "title": "macros are not recognised in dbt",
            "body": "<pre><code>{{ \n    config (\n        pre_hook = before_begin(&quot;{{audit_tbl_insert(1,'stg_news_sentiment_analysis_incr') }}&quot;),\n        post_hook = after_commit(&quot;{{audit_tbl_update(1,'stg_news_sentiment_analysis_incr','dbt_development','news_sentiment_analysis') }}&quot;)\n        )\n}}\n\nselect rd.news_id ,rd.title, rd.description, ns.sentiment from live_crawler_output_rss.rss_data rd \nleft join \nlive_crawler_output_rss.news_sentiment ns \non rd.news_id = ns.data_id limit 10000;\n</code></pre>\n<p>This is my model in DBT which is configured with pre and post hooks which referance a macro to insert and update the audit table.</p>\n<p>my macro</p>\n<pre><code>{ % macro audit_tbl_insert (model_id_no, model_name_txt) % }\n\n{% set run_id_value = var('run_id') %}\n\ninsert into {{audit_schema_name}}.{{audit_table_name}} (run_id, model_id, model_name, status, start_time, last_updated_at)\nvalues \n({{run_id_value}}::bigint,{{model_id_no}}::bigint,{{model_name_txt}},'STARTED',current_timestamp,current_timestamp)\n\n{% endmacro %}\n\n</code></pre>\n<p>this is the first time i'm using this macro and I see the following error.</p>\n<pre><code>Compilation Error in model stg_news_sentiment_analysis_incr \n(models/staging/stg_news_sentiment_analysis_incr.sql)\n'audit_tbl_insert' is undefined in macro run_hooks (macros/materializations/hooks.sql) \ncalled by macro materialization_table_default (macros/materializations/models/table/table.sql) called by model stg_news_sentiment_analysis_incr \n(models/staging/stg_news_sentiment_analysis_incr.sql). \nThis can happen when calling a macro that does not exist. \nCheck for typos and/or install package dependencies with &quot;dbt deps&quot;.\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 9478316,
                "reputation": 646,
                "user_id": 7047037,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/10207546524685823/picture?type=large",
                "display_name": "Catarina Ribeiro",
                "link": "https://stackoverflow.com/users/7047037/catarina-ribeiro"
            },
            "is_answered": true,
            "view_count": 2366,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1688957409,
            "creation_date": 1667490609,
            "question_id": 74305917,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74305917/how-do-i-test-only-the-seeds-in-my-dbt-project",
            "title": "How do I test only the seeds in my dbt project?",
            "body": "<p>How can I test only the seeds folder on my dbt projet??\n<a href=\"https://i.sstatic.net/Zb2pt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Zb2pt.png\" alt=\"enter image description here\" /></a>\n<a href=\"https://i.sstatic.net/vDhWa.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vDhWa.png\" alt=\"enter image description here\" /></a></p>\n<p>I've used <code>dbt test</code>, but it tests the entire project (seeds included). I've used <code>dbt test --select seeds</code> but it says: <code>Nothing to do. Try checking your model configs and model specification args</code></p>\n<p>Is there any way to test only the seeds folder?</p>\n<p>I've searched everywhere but I couldn't find anything regarding this</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 1012735,
                "reputation": 87,
                "user_id": 1024937,
                "user_type": "registered",
                "accept_rate": 60,
                "profile_image": "https://www.gravatar.com/avatar/3cfaf6e0a58397bf7f5217cab56241b5?s=256&d=identicon&r=PG",
                "display_name": "Silent",
                "link": "https://stackoverflow.com/users/1024937/silent"
            },
            "is_answered": true,
            "view_count": 1835,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1704291396,
            "creation_date": 1665738173,
            "question_id": 74066863,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/74066863/how-to-fill-owner-in-dbt-docs-ui",
            "title": "How to fill owner in dbt docs UI?",
            "body": "<p>I want to set the owner for dbt models from yml file. I have tried everything.</p>\n<p><a href=\"https://i.sstatic.net/hPgny.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/hPgny.png\" alt=\"enter image description here\" /></a></p>\n<pre class=\"lang-yaml prettyprint-override\"><code>---\nversion: 2\n\nmodels:\n  - name: my_model\n    description: &quot;Foo bar&quot;\n    owner: 'XXXX'  # not work\n    config:\n       owner: 'XXXX' # not work\n    meta:\n       owner: 'XXXX' # not work\n</code></pre>\n<p>dbt-core 1.2.2</p>\n"
        },
        {
            "tags": [
                "sql",
                "dbt"
            ],
            "owner": {
                "account_id": 25437822,
                "reputation": 41,
                "user_id": 19241282,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GijRyAc6xCFzXH7cdmxRve_mGqiAqR0lCGVSXkn=k-s256",
                "display_name": "Dug",
                "link": "https://stackoverflow.com/users/19241282/dug"
            },
            "is_answered": true,
            "view_count": 4383,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1733250475,
            "creation_date": 1654008525,
            "last_edit_date": 1654112382,
            "question_id": 72449955,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72449955/expressing-complex-logic-in-case-statements-in-dbt-using-a-lookup-table",
            "title": "Expressing complex logic in case statements in dbt using a lookup table",
            "body": "<p>I'm reworking a good part of our Analytics Data Warehouse and am trying to build things out in a much more modular way. We've swapped to DBT vs an in house transformation tool and I'm trying to take advantage of the functionality it offers.</p>\n<p>Previously, the way we classified our rental segments was in a series of CASE statements which evaluate a few fields. These are (sudocode)</p>\n<pre><code>CASE WHEN rental_rule_type &lt;&gt; monthly \nAND rental_length BETWEEN 6 AND 24\nAND rental_day IN (0,1,2,3,4)\nAND rental_starts IN (5,6,7,8,9,10,11)\nTHEN weekday_daytime_rental\n</code></pre>\n<p>This obviously works. But it's ugly and hard to update. If we want to adjust this, we'll need to do so in the SQL rather than in a lookup table.</p>\n<p>What I'd like to build is a simple lookup table that holds these values that can be adjusted at a later date to easily adjust how we classify these rentals, but I'm not sure what the best approach is.</p>\n<p>My current thought is to layer these conditions into an excel file, load it into the warehouse with DBT and then join on these conditions, however I'm not sure if that would end up being cleaner logic or not. It would mean there are no hardcoded values in the code, but it would likely still result in a ton of ugly cases and joins.</p>\n<p>I think there are some global variables I could define as well in DBT which may help with this?</p>\n<p>Anyone approach something similar? Would love to hear some best practices.</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 24619873,
                "reputation": 41,
                "user_id": 18520400,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxnnTPQ1Hc9KVjq9ABj6KzUPnR8WfTkKvPJ02TZ=k-s256",
                "display_name": "Ofer Elrom",
                "link": "https://stackoverflow.com/users/18520400/ofer-elrom"
            },
            "is_answered": true,
            "view_count": 2552,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1689068956,
            "creation_date": 1647782136,
            "last_edit_date": 1663742108,
            "question_id": 71547080,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71547080/dbt-recursionerror",
            "title": "DBT RecursionError",
            "body": "<p>I've installed <code>dbt</code> on my Windows PC.\nAfter creating the necessary configuration, I've executed <code>dbt debug</code> successfully.\nI tried executing <code>dbt run</code>. I have a local Postgres installed, and I'm getting the following error:</p>\n<p><code>RecursionError: maximum recursion depth exceeded while calling a Python object</code></p>\n<h5>Output of <code>dbt --version</code></h5>\n<pre><code>installed version: 1.0.4\n   latest version: 1.0.4\n\nUp to date!\n\nPlugins:\n  - postgres: 1.0.4 - Up to date!\n</code></pre>\n<p>Anything I can do?</p>\n"
        },
        {
            "tags": [
                "shell",
                "airflow",
                "dbt"
            ],
            "owner": {
                "account_id": 3405277,
                "reputation": 1285,
                "user_id": 2856499,
                "user_type": "registered",
                "accept_rate": 43,
                "profile_image": "https://www.gravatar.com/avatar/df2da7fb563267a868a90649c824a2bc?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Adam",
                "link": "https://stackoverflow.com/users/2856499/adam"
            },
            "is_answered": true,
            "view_count": 19098,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1678336917,
            "creation_date": 1636027879,
            "last_edit_date": 1678336917,
            "question_id": 69839049,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69839049/how-do-we-set-os-environment-variables-in-airflow",
            "title": "How do we set OS environment variables in Airflow",
            "body": "<p>I have an airflow DAG and what i am trying to do is read my variables stored in the airflow UI (username and password) and pass those variable values as exported values in the OS. The reason for this, is because I am using dbt yaml file which requires me to read the environment variable <code>dbt_user</code>. (the only other way is to set the password in the yaml file which is not secure.</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>default:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: xxxx\n      user: &quot;{{ env_var('dbt_user') }}&quot;\n</code></pre>\n<p>I tried to write a dag which does the bashoperator export but it does not seem to set the environment variable.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator, BranchPythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\nimport os\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2020,8,1),\n    'retries': 0\n}\n\n\nwith DAG('sample', default_args=default_args, schedule_interval='@once') as dag:\n    task_1 = BashOperator(\n        task_id='get_variables',\n        bash_command='export dbt_user={{ var.value.dbt_user }} ',\n        env = os.environ.copy(),\n        dag=dag\n    )\n\n    task_2 = BashOperator(\n        task_id='load_seed_data_once',\n        bash_command='echo $dbt_user',\n        dag=dag\n    )\n\ntask_1 &gt;&gt; task_2\n</code></pre>\n<p>when I tried to echo we can see nothing is being set. Does anyone know how to set an environment variable using the bashoperator?</p>\n<pre><code>[2021-11-04 12:00:34,452] {subprocess.py:63} INFO - Running command: ['bash', '-c', 'echo $dbt_user']\n[2021-11-04 12:00:34,463] {subprocess.py:74} INFO - Output:\n[2021-11-04 12:00:34,464] {subprocess.py:78} INFO - \n[2021-11-04 12:00:34,465] {subprocess.py:82} INFO - Command exited with return code 0\n[2021-11-04 12:00:34,494] {taskinstance.py:1211} INFO - Marking task as SUCCESS. dag_id=sample, task_id=load_seed_data_once, execution_date=20211104T120032, start_date=20211104T120034, end_date=20211104T120034\n[2021-11-04 12:00:34,517] {taskinstance.py:1265} INFO - 0 downstream tasks scheduled from follow-on schedule check\n[2021-11-04 12:00:34,555] {local_task_job.py:149} INFO - Task exited with return code 0\n</code></pre>\n<p>update:</p>\n<p>I also tried doing via the python operator but it didnt work as well. It gave me a     raise KeyError(key) from None\nKeyError: 'variable_1'</p>\n<pre class=\"lang-py prettyprint-override\"><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator, BranchPythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\nimport os\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2020,8,1),\n    'retries': 0\n}\n\ndef set_env():\n    os.environ[&quot;variable_1&quot;] = &quot;value_1&quot;\n\ndef print_env_var():\n    print(os.environ[&quot;variable_1&quot;])\n\n\nwith DAG('sample', default_args=default_args, schedule_interval='@once') as dag:\n    set_env_task = PythonOperator(\n        task_id='python_task', \n        python_callable=set_env,\n        dag=dag\n    )\n\n    print_env_task = PythonOperator(\n        task_id='load_seed_data_once',\n        python_callable=print_env_var,\n        dag=dag\n    )\n\nset_env_task &gt;&gt; print_env_task\n</code></pre>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 19173924,
                "reputation": 138,
                "user_id": 14007029,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/820a1857421f0da2fe7a6e7826aea40f?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "joellabes",
                "link": "https://stackoverflow.com/users/14007029/joellabes"
            },
            "is_answered": true,
            "view_count": 3245,
            "accepted_answer_id": 69655584,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1634787504,
            "creation_date": 1634787141,
            "question_id": 69655537,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69655537/dbt-run-select-x-gives-error-could-not-find-selector-named-x-expected-one",
            "title": "`dbt run --select x` gives error `Could not find selector named x, expected one of []`",
            "body": "<p>Using dbt 0.20.x and below, the command <code>dbt run --select model_name</code> fails.</p>\n<p>It shows the error <code>Runtime Error. Could not find selector named model_name, expected one of [] Code: 10001</code>.</p>\n"
        },
        {
            "tags": [
                "sql",
                "google-bigquery",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 20340092,
                "reputation": 43,
                "user_id": 14919645,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/1febaf5c49d463b11dff2602ec8f87ca?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "jliebss",
                "link": "https://stackoverflow.com/users/14919645/jliebss"
            },
            "is_answered": true,
            "view_count": 7105,
            "accepted_answer_id": 73254713,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1734543170,
            "creation_date": 1659707826,
            "last_edit_date": 1659707981,
            "question_id": 73250933,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/73250933/how-do-i-run-sql-model-in-dbt-multiple-times-by-looping-through-variables",
            "title": "How do I run SQL model in dbt multiple times by looping through variables?",
            "body": "<p>I have a model in dbt (<strong>test_model</strong>) that accepts a geography variable (<em>zip</em>, <em>state</em>, <em>region</em>) in the configuration. I would like to run the model three times by looping through the variables, each time running it with a different variable.</p>\n<p>Here's the catch: I have a macro shown below that appends the variable to the end of the output table name (i.e., running <strong>test_model</strong> with <em>zip</em> as the variable outputs a table called <strong>test_model_zip</strong>). This is accomplished by adding <code>{{ config(alias=var('geo')) }}</code> at the top of the model.</p>\n<p>Whether I define the variable within dbt_project.yml, the model itself, or on the CLI, I've been unable to find a way to loop through these variables, each time passing the new variable to the configuration, and successfully create three tables. Do any of you have an idea how to accomplish this? FWIW, I'm using BigQuery SQL.</p>\n<p>The macro:</p>\n<pre><code>{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n\n    {%- if custom_alias_name is none -%}\n\n        {{ node.name }}\n\n    {%- else -%}\n\n        {% set node_name = node.name ~ '_' ~ custom_alias_name %}\n        {{ node_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n</code></pre>\n<p>The model, run by entering <code>dbt run --select test_model.sql --vars '{&quot;geo&quot;: &quot;zip&quot;}'</code> in the CLI:</p>\n<pre><code>{{ config(materialized='table', alias=var('geo')) }}\n\nwith query as (select 1 as id)\n\nselect * from query\n</code></pre>\n<p>The current output: a single table called <strong>test_model_zip</strong>.</p>\n<p>The desired output: three tables called <strong>test_model_zip</strong>, <strong>test_model_state</strong>, and <strong>test_model_region</strong>.</p>\n"
        },
        {
            "tags": [
                "sql",
                "macros",
                "jinja2",
                "dbt"
            ],
            "owner": {
                "account_id": 6699569,
                "reputation": 157,
                "user_id": 5166548,
                "user_type": "registered",
                "profile_image": "https://i.sstatic.net/g5zz3.jpg?s=256",
                "display_name": "Marcos Rull&#225;n",
                "link": "https://stackoverflow.com/users/5166548/marcos-rull%c3%a1n"
            },
            "is_answered": true,
            "view_count": 3050,
            "accepted_answer_id": 72115350,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1724045874,
            "creation_date": 1651479716,
            "question_id": 72084073,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/72084073/declare-var-in-other-macro-to-be-used-in-a-different-macro-for-dbt",
            "title": "Declare var in other macro to be used in a different macro for DBT",
            "body": "<p>The main idea is to have a constant variable and then have the possibility to use it in other parts of the code or macros for DBT.</p>\n<p>Example of a macro that contains constants:</p>\n<pre><code>{% macro constant_vars() -%}\n    \n{%\nset var_1 = {\n &quot;0&quot;: [&quot;0&quot;],\n &quot;1&quot;: [&quot;1&quot;, &quot;11&quot;, &quot;111&quot;]\n}\n%}\n\n{%\nset var_2 = {\n &quot;2&quot;: [&quot;2&quot;],\n &quot;3&quot;: [&quot;3&quot;]\n}\n%}\n{%- endmacro -%}\n</code></pre>\n<p>Macro that use a constant from the previous macro:</p>\n<pre><code>{% macro evaluate(\n    column_to_check\n) -%}\n\nCASE\n    {% for map_key in var_1 -%}\n    WHEN ({{column_to_check}} IN UNNEST( {{ var_1[map_key] }})) THEN\n        '{{ map_key }}'\n    {% endfor -%}\n    ELSE\n        &quot;-1&quot;\nEND\n{%- endmacro -%}\n</code></pre>\n<p>SQL sentence created for DBT:</p>\n<pre><code>SELECT \n[..]\n  evaluate(column1)\n[..]\nFROM\n table\n</code></pre>\n<p>DBT compiled query:</p>\n<pre><code>SELECT \n[..]\nCASE\n    WHEN (column1 IN UNNEST([&quot;0&quot;])) THEN\n        '0'\n    WHEN (column1 IN UNNEST([&quot;1&quot;, &quot;11&quot;, &quot;111&quot;])) THEN\n        '1'\n    \n    ELSE\n        &quot;-1&quot;\nEND\n[..]\nFROM\n table\n</code></pre>\n<p>Is it possible? Exist another way to do that?</p>\n<p>Thanks!</p>\n"
        },
        {
            "tags": [
                "dbt"
            ],
            "owner": {
                "account_id": 1685423,
                "reputation": 2859,
                "user_id": 1548839,
                "user_type": "registered",
                "accept_rate": 65,
                "profile_image": "https://i.sstatic.net/sJWHl.jpg?s=256",
                "display_name": "Moseleyi",
                "link": "https://stackoverflow.com/users/1548839/moseleyi"
            },
            "is_answered": true,
            "view_count": 20516,
            "accepted_answer_id": 71924525,
            "answer_count": 3,
            "score": 4,
            "last_activity_date": 1674817347,
            "creation_date": 1650352330,
            "question_id": 71920945,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/71920945/env-var-required-but-not-provided-dbt-cli",
            "title": "Env var required but not provided - dbt CLI",
            "body": "<p>We have an environment variable set in dbtCloud called <code>DBT_SNOWFLAKE_ENV</code> that selects the right database depending on which environment is used.</p>\n<p>At the moment, I'm trying to set up dbt CLI with VSCode. I created a <code>profiles.yml</code> file that looks like this:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>default:\n  target: development\n  outputs:\n    development:\n      type: snowflake\n      account: skxxxx.eu-central-1\n\n      user: &lt;name&gt;\n      password: &lt;pass&gt;\n\n      role: sysadmin\n      warehouse: transformations_dw\n      database: &quot; {{ env_var('DBT_SNOWFLAKE_ENV', 'analytics_dev') }} &quot;\n      schema: transformations\n      threads: 4\n</code></pre>\n<p>I added the <code>env_var</code> line after some suggestions but I realise that the environment variable still doesn't exist yet. The problem I see is that if I hardcode <code>analytics_dev</code> in that place (which makes sense), the error still persists.</p>\n<p>I wouldn't want anybody who's going to use dbt to have to change the environment variable if they want to run something on production.</p>\n<p>What are my options here?</p>\n"
        },
        {
            "tags": [
                "sql",
                "jinja2",
                "snowflake-cloud-data-platform",
                "dbt",
                "snowflake-connector"
            ],
            "owner": {
                "account_id": 17775413,
                "reputation": 98,
                "user_id": 12907887,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/6560ad22058b5da93dc8a2dc1e892f67?s=256&d=identicon&r=PG&f=y&so-version=2",
                "display_name": "Lewis Brogan",
                "link": "https://stackoverflow.com/users/12907887/lewis-brogan"
            },
            "is_answered": true,
            "view_count": 1628,
            "accepted_answer_id": 70615757,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1641521031,
            "creation_date": 1641492952,
            "question_id": 70611726,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/70611726/using-data-build-tooldbt-and-snowflake-how-can-i-check-if-a-column-is-a-date",
            "title": "Using data build tool(dbt) and snowflake, how can I check if a column is a date field?",
            "body": "<p><strong>I'm a Junior so apologies if my explanation isn't that great.</strong></p>\n<p>I've created a macro on dbt to add a default row with defined values or default values based on data type.</p>\n<p>What I'm trying to achieve is to check if the column is a datatype <code>date</code> field, then it will return the default variable <code>{{ date_vi }}</code> which I've defined as <code>'1900-00-00'</code>, but I'm getting an error:</p>\n<p><code>dbt.adapters.snowflake.column.SnowflakeColumn object' has no attribute 'isdate</code> which tells me there is no <code>is_date()</code> which is confusing because <code>is_date()</code> works on snowflake normally.</p>\n<p>I have now noticed on the dbt docs:</p>\n<p><a href=\"https://docs.getdbt.com/reference/dbt-classes#column\" rel=\"nofollow noreferrer\">https://docs.getdbt.com/reference/dbt-classes#column</a></p>\n<p>and the source code on github for snowflake:</p>\n<p><a href=\"https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/adapters/snowflake/column.py\" rel=\"nofollow noreferrer\">https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/adapters/snowflake/column.py</a></p>\n<p>That is_date() isn't actually available with the snowflake adapter, the code I was trying to get working was: <code>{% elif col.is_date() %}{{ date_vl }}</code> so I'm wondering what would be the be best way to check if a column is a date <code>datatype</code>? Hopefully I explained it enough as I'm still fairly new.</p>\n<p>Cheers.</p>\n"
        },
        {
            "tags": [
                "snowflake-cloud-data-platform",
                "dbt",
                "database-permissions",
                "fivetran"
            ],
            "owner": {
                "account_id": 452750,
                "reputation": 4473,
                "user_id": 849354,
                "user_type": "registered",
                "accept_rate": 62,
                "profile_image": "https://www.gravatar.com/avatar/21d92e60aa7179788093b60c029fe963?s=256&d=identicon&r=PG",
                "display_name": "LittleBobbyTables",
                "link": "https://stackoverflow.com/users/849354/littlebobbytables"
            },
            "is_answered": true,
            "view_count": 2071,
            "accepted_answer_id": 69272788,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1632243597,
            "creation_date": 1632242680,
            "question_id": 69272609,
            "content_license": "CC BY-SA 4.0",
            "link": "https://stackoverflow.com/questions/69272609/grant-access-to-a-specific-future-table-in-snowflake",
            "title": "Grant access to a specific future table in Snowflake",
            "body": "<p>I'm currently using Fivetran to pipe data into Snowflake. Once the data has landed, every 6 hours DBT runs some simple saved queries, which appear as tables in a schema.\nHowever, the permissions for various roles keep being reset and they can no longer access the tables in that schema that I gave them permission to see. I suspect this is because DBT is dropping and then re-creating the tables in question.</p>\n<p>One possible solution is to grant access to future tables in the schema, e.g.:</p>\n<pre><code>grant select on future tables in schema myschema to role some_role;\n</code></pre>\n<p>However, I just want to give access to a single table, not all. Is that possible?</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 296
}